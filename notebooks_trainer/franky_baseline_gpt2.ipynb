{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import safetensors\n",
    "\n",
    "\n",
    "import einops\n",
    "\n",
    "from models import brainformer\n",
    "from utils.data_utils import BrainDataset, get_tokenizer\n",
    "from utils.train_utils import TrainConfig, run_train_model, count_parameters, simple_train_model\n",
    "\n",
    "\n",
    "from models.brainformer import Encoder, CrossBlock, build_complex_rope_cache, Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from models.gpt2_model import GPT\n",
    "import tiktoken\n",
    "from contextlib import nullcontext\n",
    "from accelerate import notebook_launcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate.utils import write_basic_config\n",
    "\n",
    "# write_basic_config()  # Write a config file\n",
    "# os._exit(00)  # Restart the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainEncoder(nn.Module): \n",
    "    config = Config\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.encoder = Encoder(config.encoder)\n",
    "        self.n_output_tokens = config.n_output_tokens\n",
    "\n",
    "        self.learnable_queries = nn.Parameter(torch.zeros(1, config.n_output_tokens, config.dim))\n",
    "        self.perceiver = nn.ModuleDict(dict(\n",
    "                h = nn.ModuleList([CrossBlock(config) for _ in range(config.n_layers)]),\n",
    "                ln_f = nn.LayerNorm(config.dim), \n",
    "                to_words = nn.Linear(config.dim, config.output_dim))\n",
    "        )\n",
    "        \n",
    "        self.register_buffer('cross_attn_mask', None)\n",
    "        self.register_buffer('self_attn_mask', None)\n",
    "\n",
    "        self.precompute_rope_cash = build_complex_rope_cache(dim=config.head_dim,\n",
    "                                                             seq_len=config.n_output_tokens,\n",
    "                                                             theta=config.rope_theta)\n",
    "        # self.cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        print(\"Full HandFormer: number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "    \n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return next(self.parameters()).dtype\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    @property\n",
    "    def rope_cache(self) -> torch.Tensor:\n",
    "        # Just to use proper device.\n",
    "        if self.precompute_rope_cash.device != self.device:\n",
    "            self.precompute_rope_cash = self.precompute_rope_cash.to(device=self.device)\n",
    "        return self.precompute_rope_cash                \n",
    "    \n",
    "    def forward(self, x, targets=None, date_info=None):\n",
    "        \"\"\"\n",
    "        Get forward pass with loss calculation.\n",
    "        Inputs: \n",
    "        x\n",
    "            shape b t c \n",
    "        targets:\n",
    "            B, C, T\n",
    "        \"\"\"\n",
    "        b, t, c = x.shape\n",
    "\n",
    "        emg_context = self.encoder(x) # b, n_tokens, dim\n",
    "        \n",
    "        input = self.learnable_queries.expand(b, self.n_output_tokens, -1)\n",
    "        \n",
    "        for cross_block in self.perceiver.h:\n",
    "            input = cross_block(input, emg_context, self.self_attn_mask, \n",
    "                                self.cross_attn_mask, sa_rope = self.rope_cache)\n",
    "        \n",
    "        logits = self.perceiver.ln_f(input)\n",
    "        logits = self.perceiver.to_words(logits)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Franky(nn.Module): \n",
    "    \"\"\"This is first model which incorporate brain features into LLM\"\"\"\n",
    "\n",
    "    def __init__(self, brain_model, llm_model, tokenizer=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.brain_model = brain_model\n",
    "        self.llm_model= llm_model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        print(\"Full Franky: number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "    \n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return next(self.parameters()).dtype\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, x, targets=None, date_info=None):\n",
    "        \"\"\"\n",
    "        Train model.\n",
    "        \"\"\"\n",
    "        features = self.brain_model(x)\n",
    "\n",
    "        new_idx = targets.clone()\n",
    "        new_idx[new_idx == -100] = 50256\n",
    "\n",
    "        loss, logits = self.llm_model.forward(idx=new_idx, prefix=features, targets=targets)\n",
    "\n",
    "        return loss, logits\n",
    "    \n",
    "    def generate(self, x, date_info=None):\n",
    "        \n",
    "        prefix = self.brain_model(x)\n",
    "        \n",
    "        start = '<|endoftext|>'\n",
    "        input_ids = self.tokenizer(start,  return_tensors=\"pt\")['input_ids']\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        \n",
    "        max_new_tokens = 15\n",
    "        temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "        top_k = 20\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y = self.llm_model.generate(x, max_new_tokens, prefix=prefix, temperature=temperature, top_k=top_k)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n",
      "Encoder: number of parameters: 4.27M\n",
      "Shape of casual mask:  torch.Size([6144, 6144])\n",
      "Shape of the rope cache:  torch.Size([6144, 16])\n",
      "Full HandFormer: number of parameters: 6.32M\n",
      "Full Franky: number of parameters: 130.76M\n",
      "Initing of the Franky completed\n",
      "Total: 130.76M, Trainable: 6.32M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(130757888, 6318080)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "dtype = torch.float32\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "llm_model = GPT.from_pretrained('gpt2', dict(dropout=0.0))\n",
    "llm_model.eval()\n",
    "\n",
    "for param in llm_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "mae_config = brainformer.MAEConfig(window_size=768, patch_size=32)\n",
    "config = brainformer.Config(encoder=mae_config,\n",
    "                            n_output_tokens=32,\n",
    "                            output_dim=llm_model.config.n_embd\n",
    "                            )\n",
    "brain_model = BrainEncoder(config)\n",
    "\n",
    "\n",
    "### Create Franky model\n",
    "model = Franky(brain_model=brain_model, llm_model=llm_model)\n",
    "model.train().to(torch.float32).to(device)\n",
    "\n",
    "print('Initing of the Franky completed')\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import safetensors\n",
    "\n",
    "# weights = Path(\"/drive/logs/kovalev/franky_gpt2_retrain/step_5000_loss_3.1739.safetensors\")\n",
    "\n",
    "# safetensors.torch.load_model(model, weights)\n",
    "\n",
    "# model.brain_model.state_dict()['learnable_queries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runed processing of the  D:\\Work\\brain-to-text-competition\\data\\competitionData\\test\n",
      "bad_samples [15, 17, 18, 22]\n",
      "Runed processing of the  D:\\Work\\brain-to-text-competition\\data\\competitionData\\test\n",
      "bad_samples [15, 17, 18, 22]\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(r\"D:\\Work\\brain-to-text-competition\\data\\competitionData\")\n",
    "# data_path = Path(\"/drive/data/competitionData\")\n",
    "\n",
    "train_dataset = BrainDataset(data_path / 'test', tokenize_function=get_tokenizer(tokenizer))\n",
    "test_dataset = BrainDataset(data_path / 'test', tokenize_function=get_tokenizer(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top3 scores:  tensor(-28.4133, device='cuda:0') tensor(-28.4499, device='cuda:0') tensor(-28.5924, device='cuda:0')\n",
      "Top3 scores:  [50256, 198, 11, 290, 262, 366, 82, 13, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 1, 318] [50256, 198, 11, 290, 262, 366, 82, 13, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 818, 262] [50256, 198, 11, 290, 262, 366, 82, 13, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 40, 423]\n",
      "[50256, 198, 11, 290, 262, 366, 82, 13, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 1, 318]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'nonzero'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mllm_model\u001b[38;5;241m.\u001b[39mbeam_search(input_ids, max_new_tokens, prefix\u001b[38;5;241m=\u001b[39mprefix, temperature\u001b[38;5;241m=\u001b[39mtemperature, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n\u001b[1;32m---> 22\u001b[0m idxs \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m()\n\u001b[0;32m     24\u001b[0m start, end \u001b[38;5;241m=\u001b[39m idxs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, idxs[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     25\u001b[0m idxs_clean \u001b[38;5;241m=\u001b[39m y[start:end]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'nonzero'"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[2]\n",
    "\n",
    "x = sample[0]\n",
    "x = torch.from_numpy(x[None, ]).to(device)\n",
    "\n",
    "        \n",
    "start = '<|endoftext|>'\n",
    "input_ids = tokenizer(start,  return_tensors=\"pt\")['input_ids']\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "max_new_tokens = 25\n",
    "temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 10\n",
    "\n",
    "for k in range(2):\n",
    "    with torch.no_grad():\n",
    "        prefix = model.brain_model(x)\n",
    "\n",
    "        y = model.llm_model.beam_search(input_ids, max_new_tokens, prefix=prefix, temperature=temperature, beam_width=5)\n",
    "\n",
    "        print(y)\n",
    "        idxs = (y == 50256).nonzero()\n",
    "\n",
    "        start, end = idxs[0].item() + 1, idxs[1].item()\n",
    "        idxs_clean = y[start:end]\n",
    "        pred = tokenizer.decode(idxs_clean, skip_special_tokens=False)\n",
    "\n",
    "        print(pred)\n",
    "\n",
    "\n",
    "labels = sample[1]\n",
    "labels[labels == -100] = tokenizer.bos_token_id\n",
    "\n",
    "idxs = (labels == 50256).nonzero()[0]\n",
    "start, end = idxs[0] + 1, idxs[1]\n",
    "\n",
    "labels = labels[start: end]\n",
    "\n",
    "gt = tokenizer.decode(labels)\n",
    "\n",
    "print('-------')\n",
    "print(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = '<|endoftext|>i love you so much <|endoftext|>'\n",
    "\n",
    "# input_ids = tokenizer(start,  return_tensors=\"pt\")['input_ids']\n",
    "# input_ids = input_ids.to(device)\n",
    "\n",
    "# brain_activity = torch.randn(1, 768, 256, dtype=torch.float32, device=device)\n",
    "\n",
    "# loss, _ = model.forward(brain_activity, targets=input_ids)\n",
    "\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'frankenstein'\n",
    "\n",
    "train_config = TrainConfig(exp_name='franky_gpt2_retrain',\n",
    "                           mixed_precision=True, \n",
    "                           batch_size=32, \n",
    "                           num_workers=3, \n",
    "                           pin_memory=True, \n",
    "                           eval_interval=500)\n",
    "# peter path\n",
    "# data_path = Path(r'C:\\Users\\peter\\alvi\\brain2text\\competitionData')\n",
    "data_path = Path(\"/drive/data/competitionData\")\n",
    "save_folder = Path(\"/drive/logs/kovalev\")\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = BrainDataset(data_path / 'train', tokenize_function=get_tokenizer(tokenizer))\n",
    "test_dataset = BrainDataset(data_path / 'test', tokenize_function=get_tokenizer(tokenizer))\n",
    "\n",
    "\n",
    "args = (model, (train_dataset, test_dataset), train_config, project_name, save_folder)\n",
    "notebook_launcher(run_train_model, args, num_processes=1)\n",
    "\n",
    "# simple_train_model(*args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
