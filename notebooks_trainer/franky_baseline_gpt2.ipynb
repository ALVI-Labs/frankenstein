{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import safetensors\n",
    "\n",
    "\n",
    "import einops\n",
    "\n",
    "from models import brainformer\n",
    "from utils.data_utils import BrainDataset, get_tokenizer\n",
    "from utils.train_utils import TrainConfig, run_train_model, count_parameters, simple_train_model\n",
    "\n",
    "\n",
    "from models.brainformer import Encoder, CrossBlock, build_complex_rope_cache, Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from models.gpt2_model import GPT\n",
    "import tiktoken\n",
    "from contextlib import nullcontext\n",
    "from accelerate import notebook_launcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate.utils import write_basic_config\n",
    "\n",
    "# write_basic_config()  # Write a config file\n",
    "# os._exit(00)  # Restart the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainEncoder(nn.Module): \n",
    "    config = Config\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.encoder = Encoder(config.encoder)\n",
    "        self.n_output_tokens = config.n_output_tokens\n",
    "\n",
    "        self.learnable_queries = nn.Parameter(torch.zeros(1, config.n_output_tokens, config.dim))\n",
    "        self.perceiver = nn.ModuleDict(dict(\n",
    "                h = nn.ModuleList([CrossBlock(config) for _ in range(config.n_layers)]),\n",
    "                ln_f = nn.LayerNorm(config.dim), \n",
    "                to_words = nn.Linear(config.dim, config.output_dim))\n",
    "        )\n",
    "        \n",
    "        self.register_buffer('cross_attn_mask', None)\n",
    "        self.register_buffer('self_attn_mask', None)\n",
    "\n",
    "        self.precompute_rope_cash = build_complex_rope_cache(dim=config.head_dim,\n",
    "                                                             seq_len=config.n_output_tokens,\n",
    "                                                             theta=config.rope_theta)\n",
    "        # self.cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        print(\"Full HandFormer: number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "    \n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return next(self.parameters()).dtype\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    @property\n",
    "    def rope_cache(self) -> torch.Tensor:\n",
    "        # Just to use proper device.\n",
    "        if self.precompute_rope_cash.device != self.device:\n",
    "            self.precompute_rope_cash = self.precompute_rope_cash.to(device=self.device)\n",
    "        return self.precompute_rope_cash                \n",
    "    \n",
    "    def forward(self, x, targets=None, date_info=None):\n",
    "        \"\"\"\n",
    "        Get forward pass with loss calculation.\n",
    "        Inputs: \n",
    "        x\n",
    "            shape b t c \n",
    "        targets:\n",
    "            B, C, T\n",
    "        \"\"\"\n",
    "        b, t, c = x.shape\n",
    "\n",
    "        emg_context = self.encoder(x) # b, n_tokens, dim\n",
    "        \n",
    "        input = self.learnable_queries.expand(b, self.n_output_tokens, -1)\n",
    "        \n",
    "        for cross_block in self.perceiver.h:\n",
    "            input = cross_block(input, emg_context, self.self_attn_mask, \n",
    "                                self.cross_attn_mask, sa_rope = self.rope_cache)\n",
    "        \n",
    "        logits = self.perceiver.ln_f(input)\n",
    "        logits = self.perceiver.to_words(logits)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Franky(nn.Module):\n",
    "    \"\"\"This is first model which incorporate brain features into LLM\"\"\"\n",
    "\n",
    "    def __init__(self, brain_model, llm_model, tokenizer=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.brain_model = brain_model\n",
    "        self.llm_model= llm_model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        print(\"Full Franky: number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "    \n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return next(self.parameters()).dtype\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, x, targets=None, date_info=None):\n",
    "        \"\"\"\n",
    "        Train model.\n",
    "        \"\"\"\n",
    "        features = self.brain_model(x)\n",
    "\n",
    "        new_idx = targets.clone()\n",
    "        new_idx[new_idx == -100] = 50256\n",
    "\n",
    "        loss, logits = self.llm_model.forward(idx=new_idx, prefix=features, targets=targets)\n",
    "\n",
    "        return loss, logits\n",
    "    \n",
    "    def generate(self, x, date_info=None, tokenizer=None):\n",
    "        device = self.device\n",
    "        \n",
    "        x = torch.from_numpy(x[None, ]).to(device)\n",
    "\n",
    "        prefix = self.brain_model(x)\n",
    "        start = '<|endoftext|>'\n",
    "        input_ids = tokenizer(start,  return_tensors=\"pt\")['input_ids']\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        \n",
    "        max_new_tokens = 25\n",
    "        temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "        top_k = 10\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y = self.llm_model.generate(x, max_new_tokens, prefix=prefix, temperature=temperature, top_k=top_k)\n",
    "\n",
    "        stop_tokens_ids = (y == 50256).nonzero()\n",
    "\n",
    "        if len(stop_tokens)==1:\n",
    "            end = len(y) if len(stop_tokens)==1 else stop_tokens_ids[1]\n",
    "\n",
    "        ids_clean = y[1:end]      \n",
    "        pred = tokenizer.decode(ids_clean, skip_special_tokens=False)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n",
      "Encoder: number of parameters: 4.27M\n",
      "Shape of casual mask:  torch.Size([6144, 6144])\n",
      "Shape of the rope cache:  torch.Size([6144, 16])\n",
      "Full HandFormer: number of parameters: 6.32M\n",
      "Full Franky: number of parameters: 130.76M\n",
      "Initing of the Franky completed\n",
      "Total: 130.76M, Trainable: 130.76M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(130757888, 130757888)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "dtype = torch.float32\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "llm_model = GPT.from_pretrained('gpt2', dict(dropout=0.0))\n",
    "llm_model.eval()\n",
    "\n",
    "for param in llm_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "mae_config = brainformer.MAEConfig(window_size=768, patch_size=32)\n",
    "config = brainformer.Config(encoder=mae_config,\n",
    "                            n_output_tokens=32,\n",
    "                            output_dim=llm_model.config.n_embd\n",
    "                            )\n",
    "brain_model = BrainEncoder(config)\n",
    "\n",
    "\n",
    "### Create Franky model\n",
    "model = Franky(brain_model=brain_model, llm_model=llm_model)\n",
    "model.train().to(torch.float32).to(device)\n",
    "\n",
    "print('Initing of the Franky completed')\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0229,  0.0101, -0.0027,  ..., -0.0247,  0.0153,  0.0005],\n",
       "         [ 0.0670, -0.0147, -0.0068,  ..., -0.0044, -0.0117, -0.0123],\n",
       "         [ 0.0188, -0.0051, -0.0379,  ...,  0.0185, -0.0410, -0.0402],\n",
       "         ...,\n",
       "         [ 0.0058,  0.0146, -0.0130,  ...,  0.0190, -0.0231,  0.0086],\n",
       "         [-0.0113,  0.0127,  0.0033,  ...,  0.0024,  0.0237,  0.0085],\n",
       "         [ 0.0214, -0.0044, -0.0212,  ...,  0.0004, -0.0257, -0.0230]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import safetensors\n",
    "\n",
    "weights = Path(\"/drive/logs/kovalev/franky_gpt2_retrain/step_5000_loss_3.1739.safetensors\")\n",
    "\n",
    "safetensors.torch.load_model(model, weights)\n",
    "\n",
    "model.brain_model.state_dict()['learnable_queries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data_path = Path(r\"D:\\Work\\brain-to-text-competition\\data\\competitionData\")\n",
    "# data_path = Path(\"/drive/data/competitionData\")\n",
    "\n",
    "# train_dataset = BrainDataset(data_path / 'train', tokenize_function=get_tokenizer(tokenizer))\n",
    "# test_dataset = BrainDataset(data_path / 'test', tokenize_function=get_tokenizer(tokenizer))\n",
    "\n",
    "# start = '<|endoftext|>i love you so much <|endoftext|>'\n",
    "\n",
    "# input_ids = tokenizer(start,  return_tensors=\"pt\")['input_ids']\n",
    "# input_ids = input_ids.to(device)\n",
    "\n",
    "# brain_activity = torch.randn(1, 768, 256, dtype=torch.float32, device=device)\n",
    "\n",
    "# loss, _ = model.forward(brain_activity, targets=input_ids)\n",
    "\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runed processing of the  /drive/data/competitionData/train\n",
      "bad_samples [31, 32, 33, 37, 40, 41, 42, 43, 44, 47, 51, 56, 59, 61, 64, 68, 69, 79, 81, 88, 91, 92, 100, 101, 102, 103, 109, 113, 116, 119, 139, 141, 142, 148, 163, 166, 175, 183, 236, 244, 270, 276, 282, 323, 334, 359, 430, 470, 484, 488, 492, 493, 498, 500, 506, 522, 623, 626]\n",
      "Runed processing of the  /drive/data/competitionData/test\n",
      "bad_samples [15, 17, 18, 22]\n",
      "Launching training on one GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(split_batches=True)\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkoval_alvi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kovalev/frankenstein/notebooks_trainer/wandb/run-20240505_180511-eilj0agh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/koval_alvi/frankenstein/runs/eilj0agh' target=\"_blank\">jedi-pilot-33</a></strong> to <a href='https://wandb.ai/koval_alvi/frankenstein' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/koval_alvi/frankenstein' target=\"_blank\">https://wandb.ai/koval_alvi/frankenstein</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/koval_alvi/frankenstein/runs/eilj0agh' target=\"_blank\">https://wandb.ai/koval_alvi/frankenstein/runs/eilj0agh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device for training:  cuda\n",
      "Num devices:  1\n",
      "Completed initialization of scheduler\n",
      "********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************overall_steps 500: 3.118927240371704\n",
      "val loss: 3.2442941665649414\n",
      "saved model:  step_500_loss_3.2443.safetensors\n",
      "********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************overall_steps 1000: 2.013702630996704\n",
      "val loss: 3.7655463218688965\n",
      "********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************overall_steps 1500: 1.6659034490585327\n",
      "val loss: 4.172821521759033\n",
      "******************************************************************************************************************************************************************************************************************************************************************************"
     ]
    }
   ],
   "source": [
    "project_name = 'frankenstein'\n",
    "\n",
    "train_config = TrainConfig(exp_name='franky_unfreezed_gpt2',\n",
    "                           mixed_precision=True, \n",
    "                           batch_size=32, \n",
    "                           num_workers=3, \n",
    "                           pin_memory=True, \n",
    "                           eval_interval=500)\n",
    "# peter path\n",
    "# data_path = Path(r'C:\\Users\\peter\\alvi\\brain2text\\competitionData')\n",
    "data_path = Path(\"/drive/data/competitionData\")\n",
    "save_folder = Path(\"/drive/logs/kovalev\")\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = BrainDataset(data_path / 'train', tokenize_function=get_tokenizer(tokenizer))\n",
    "test_dataset = BrainDataset(data_path / 'test', tokenize_function=get_tokenizer(tokenizer))\n",
    "\n",
    "\n",
    "args = (model, (train_dataset, test_dataset), train_config, project_name, save_folder)\n",
    "notebook_launcher(run_train_model, args, num_processes=1)\n",
    "\n",
    "# simple_train_model(*args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
