{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import safetensors\n",
    "\n",
    "\n",
    "import einops\n",
    "\n",
    "from models import brainformer\n",
    "from utils.data_utils import BrainDataset, get_tokenizer\n",
    "from utils.train_utils import TrainConfig, run_train_model, count_parameters, simple_train_model\n",
    "\n",
    "from models.simple_mae_abs import SimpleEncoder, SimpleMAE\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from simple_parsing.helpers import Serializable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from peft import get_peft_model\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from models.gpt2_model import GPT\n",
    "import tiktoken\n",
    "from contextlib import nullcontext\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Franky(nn.Module): \n",
    "    \"\"\"This is first model which incorporate brain features into LLM\"\"\"\n",
    "\n",
    "    def __init__(self, brain_model, llm_model, tokenizer=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.brain_model = brain_model\n",
    "        self.projector = nn.Linear(self.brain_model.config.dim, llm_model.config.n_embd)\n",
    "        self.llm_model= llm_model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.date_embeddings = nn.Embedding(num_embeddings=25, embedding_dim=llm_model.config.n_embd)\n",
    "        \n",
    "        print(\"Full Franky: number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "    \n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return next(self.parameters()).dtype\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, x, targets=None, date_info=None):\n",
    "        \"\"\"\n",
    "        Train model.\n",
    "        \"\"\"\n",
    "        attn_mask = self.brain_model.get_attn_mask_padded(x)\n",
    "        attn_mask = attn_mask.unsqueeze(1)\n",
    "\n",
    "        features = self.brain_model(x, attn_mask)\n",
    "        features = features[:, :self.brain_model.n_registers]\n",
    "        \n",
    "        features = self.projector(features)\n",
    "\n",
    "        date_emb = self.date_embeddings(date_info)\n",
    "        date_emb = date_emb.unsqueeze(1)\n",
    "\n",
    "        features = torch.cat([features, date_emb], axis=1)\n",
    "\n",
    "        new_idx = targets.clone()\n",
    "        new_idx[new_idx == -100] = 50256\n",
    "\n",
    "        loss, logits = self.llm_model.forward(idx=new_idx, prefix=features, targets=targets)\n",
    "\n",
    "        return loss, logits\n",
    "    \n",
    "    def generate(self, x, date_info=None, tokenizer=None):\n",
    "        device = self.device\n",
    "        \n",
    "        x = torch.from_numpy(x[None, ]).to(device).to(self.dtype)\n",
    "\n",
    "        ### Encoder part\n",
    "        attn_mask = self.brain_model.get_attn_mask_padded(x)\n",
    "        attn_mask = attn_mask.unsqueeze(1)\n",
    "\n",
    "        features = self.brain_model(x, attn_mask)\n",
    "        features = features[:, :self.brain_model.n_registers]\n",
    "        \n",
    "        features = self.projector(features)\n",
    "\n",
    "        date_emb = self.date_embeddings(date_info)\n",
    "        date_emb = date_emb.unsqueeze(1)\n",
    "        \n",
    "        features = torch.cat([features, date_emb], axis=1)\n",
    "\n",
    "        ### Text part\n",
    "        start = '<|endoftext|>'\n",
    "        input_ids = tokenizer(start,  return_tensors=\"pt\")['input_ids'].to(self.device)\n",
    "        \n",
    "        max_new_tokens = 25\n",
    "        temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "        top_k = 10\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y = self.llm_model.generate(input_ids, max_new_tokens, prefix=features, temperature=temperature, top_k=top_k)\n",
    "\n",
    "        stop_tokens_ids = (y == 50256).nonzero()\n",
    "        end = len(y) if len(stop_tokens_ids)==1 else stop_tokens_ids[1]\n",
    "        ids_clean = y[1:end]\n",
    "        \n",
    "        pred = tokenizer.decode(ids_clean, skip_special_tokens=True)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SimpleEncoderConfig(Serializable):\n",
    "    # data params\n",
    "    block_size: int = 768\n",
    "    patch_size: int = 256\n",
    "\n",
    "    n_layers: int = 8\n",
    "    dim: int = 256\n",
    "    hidden_dim: int = 1024\n",
    "    n_registers: int = 16\n",
    "\n",
    "    head_dim: int = 32\n",
    "    n_heads: int = 16\n",
    "    n_kv_heads: int = 16 # now it should be the same with n_heads.\n",
    "    rope_theta: int = 10000\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SimpleMAEConfig(Serializable):\n",
    "    # data params\n",
    "    n_layers: int = 4\n",
    "    dim: int = 256\n",
    "    hidden_dim: int = 1024\n",
    "\n",
    "    head_dim: int = 32\n",
    "    n_heads: int = 8\n",
    "    n_kv_heads: int = 8 # now it should be the same with n_heads.\n",
    "    rope_theta: int = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleEncoderConfig(block_size=768, patch_size=256, n_layers=8, dim=256, hidden_dim=1024, n_registers=16, head_dim=32, n_heads=16, n_kv_heads=16, rope_theta=10000)\n",
      "Encoder: number of parameters: 10.76M\n",
      "MAE: number of parameters: 15.29M\n"
     ]
    }
   ],
   "source": [
    "mae_weights = \"/drive/logs/kovalev/fixed_abs_mae_11M_5M_spikes/step_34000_loss_0.0166.safetensors\"\n",
    "\n",
    "mae_model = SimpleMAE( SimpleEncoderConfig(), SimpleMAEConfig())\n",
    "mae_model = torch.compile(mae_model)\n",
    "safetensors.torch.load_model(mae_model, mae_weights)\n",
    "mae_model = mae_model._orig_mod\n",
    "\n",
    "brain_model = mae_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n",
      "Full Franky: number of parameters: 135.42M\n",
      "Initing of the Franky completed\n",
      "Total: 135.42M, Trainable: 0.22M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(135417344, 216576)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "dtype = torch.float32\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "llm_model = GPT.from_pretrained('gpt2', dict(dropout=0.0)) # \n",
    "\n",
    "### Create Franky model\n",
    "model = Franky(brain_model=brain_model, llm_model=llm_model)\n",
    "\n",
    "# weights = '/drive/logs/kovalev/fixed_franky_v2_spikes_only_projector/step_500_loss_3.4384.safetensors'\n",
    "# safetensors.torch.load_model(model, weights)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.5,\n",
    "    target_modules=[\"c_proj\", \"c_attn\", 'c_fc','c_proj'])\n",
    "\n",
    "# model = get_peft_model(model, config)\n",
    "\n",
    "# for param in model.llm_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "for param in model.projector.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.date_embeddings.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.brain_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.llm_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "print('Initing of the Franky completed')\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, Clip\n",
    "\n",
    "train_transform = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.15, p=0.5),\n",
    "    Clip(0, 1, p=1)\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    Clip(0, 1, p=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = Path(\"/drive/data/competitionData\")\n",
    "# test_dataset = BrainDataset(data_path / 'test', tokenize_function=get_tokenizer(tokenizer), transform=train_transform)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# x, y, date = next(iter(test_dataloader))\n",
    "# print(x.shape, y.shape, date)\n",
    "# y = model(x, y, date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runed processing of the  /drive/data/competitionData/train\n",
      "len: 8800\n",
      "max input len 768\n",
      "Runed processing of the  /drive/data/competitionData/test\n",
      "len: 880\n",
      "max input len 768\n",
      "Launching training on one GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(split_batches=True)\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkoval_alvi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kovalev/frankenstein/notebooks_trainer/wandb/run-20240512_152832-8tc0ta5u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/koval_alvi/frankenstein/runs/8tc0ta5u' target=\"_blank\">augs_franky_spikes_date_emb_freeze_mae_gpt</a></strong> to <a href='https://wandb.ai/koval_alvi/frankenstein' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/koval_alvi/frankenstein' target=\"_blank\">https://wandb.ai/koval_alvi/frankenstein</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/koval_alvi/frankenstein/runs/8tc0ta5u' target=\"_blank\">https://wandb.ai/koval_alvi/frankenstein/runs/8tc0ta5u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device for training:  cuda\n",
      "Num devices:  1\n",
      "Completed initialization of scheduler\n",
      "*********************************************************************************************************************************************************"
     ]
    }
   ],
   "source": [
    "project_name = 'frankenstein'\n",
    "\n",
    "train_config = TrainConfig(exp_name='augs_franky_spikes_date_emb_freeze_mae_gpt',\n",
    "                           mixed_precision=True, \n",
    "                           batch_size=256, \n",
    "                           num_workers=3, \n",
    "                           pin_memory=True, \n",
    "                           warmup_iters=1000,\n",
    "                           eval_interval=500,\n",
    "                           grad_accum=1, \n",
    "                           weight_decay=1e-2)\n",
    "# peter path\n",
    "# data_path = Path(r'C:\\Users\\peter\\alvi\\brain2text\\competitionData')\n",
    "data_path = Path(\"/drive/data/competitionData\")\n",
    "save_folder = Path(\"/drive/logs/kovalev\")\n",
    "\n",
    "train_dataset = BrainDataset(data_path / 'train', tokenize_function=get_tokenizer(tokenizer), transform=train_transform)\n",
    "test_dataset = BrainDataset(data_path / 'test', tokenize_function=get_tokenizer(tokenizer), transform=test_transform)\n",
    "\n",
    "# indices = torch.arange(32).repeat(4)\n",
    "# train_dataset = torch.utils.data.Subset(train_dataset, indices)\n",
    "# test_dataset = torch.utils.data.Subset(test_dataset, indices)\n",
    "args = (model, (train_dataset, test_dataset), train_config, project_name, save_folder)\n",
    "notebook_launcher(run_train_model, args, num_processes=1)\n",
    "\n",
    "# simple_train_model(*args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 10\n",
    "\n",
    "for i, sample in enumerate(train_dataset):\n",
    "    \n",
    "    x, gt, _ = sample\n",
    "    \n",
    "    pred = model.generate(x, tokenizer=tokenizer)\n",
    "    \n",
    "    gt[gt==-100]=50256\n",
    "    gt_txt = tokenizer.decode(gt, skip_special_tokens=True)\n",
    "    \n",
    "    print('pred: ', pred)\n",
    "    print('gt_txt: ', gt_txt)\n",
    "    print('----')\n",
    "    if i > max:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, pred = tokenizer.decode(ids_clean, skip_special_tokens=False)\n",
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(x, tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
