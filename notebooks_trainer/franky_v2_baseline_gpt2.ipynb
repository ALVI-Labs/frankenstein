{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import safetensors\n",
    "\n",
    "\n",
    "import einops\n",
    "\n",
    "from models import brainformer\n",
    "from utils.data_utils import BrainDataset, get_tokenizer\n",
    "from utils.train_utils import TrainConfig, run_train_model, count_parameters, simple_train_model\n",
    "\n",
    "from models.simple_mae_abs import SimpleEncoder, SimpleMAE\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from simple_parsing.helpers import Serializable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from peft import get_peft_model\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from models.gpt2_model import GPT\n",
    "import tiktoken\n",
    "from contextlib import nullcontext\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Franky(nn.Module): \n",
    "    \"\"\"This is first model which incorporate brain features into LLM\"\"\"\n",
    "\n",
    "    def __init__(self, brain_model, llm_model, tokenizer=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.brain_model = brain_model\n",
    "        self.projector = nn.Linear(self.brain_model.config.dim, llm_model.config.n_embd)\n",
    "        self.llm_model= llm_model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.date_embeddings = nn.Embedding(num_embeddings=25, embedding_dim=llm_model.config.n_embd)\n",
    "        \n",
    "        print(\"Full Franky: number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "    \n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return next(self.parameters()).dtype\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, x, targets=None, date_info=None):\n",
    "        \"\"\"\n",
    "        Train model.\n",
    "        \"\"\"\n",
    "        attn_mask = self.brain_model.get_attn_mask_padded(x)\n",
    "        attn_mask = attn_mask.unsqueeze(1)\n",
    "\n",
    "        features = self.brain_model(x, attn_mask)\n",
    "        # features = features[:, :self.brain_model.n_registers]\n",
    "        \n",
    "        features = self.projector(features)\n",
    "\n",
    "        date_emb = self.date_embeddings(date_info)\n",
    "        date_emb = date_emb.unsqueeze(1)\n",
    "\n",
    "        features = torch.cat([features, date_emb], axis=1)\n",
    "\n",
    "        new_idx = targets.clone()\n",
    "        new_idx[new_idx == -100] = 50256\n",
    "\n",
    "        loss, logits = self.llm_model.forward(idx=new_idx, prefix=features, targets=targets)\n",
    "\n",
    "        return loss, logits\n",
    "    \n",
    "    def generate(self, x, date_info=None, tokenizer=None):\n",
    "        device = self.device\n",
    "        \n",
    "        x = torch.from_numpy(x[None, ]).to(device).to(self.dtype)\n",
    "\n",
    "        ### Encoder part\n",
    "        attn_mask = self.brain_model.get_attn_mask_padded(x)\n",
    "        attn_mask = attn_mask.unsqueeze(1)\n",
    "\n",
    "        features = self.brain_model(x, attn_mask)\n",
    "        # features = features[:, :self.brain_model.n_registers]\n",
    "        \n",
    "        features = self.projector(features)\n",
    "\n",
    "        date_emb = self.date_embeddings(date_info)\n",
    "        date_emb = date_emb.unsqueeze(1)\n",
    "        \n",
    "        features = torch.cat([features, date_emb], axis=1)\n",
    "\n",
    "        ### Text part\n",
    "        start = '<|endoftext|>'\n",
    "        input_ids = tokenizer(start,  return_tensors=\"pt\")['input_ids'].to(self.device)\n",
    "        \n",
    "        max_new_tokens = 25\n",
    "        temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "        top_k = 10\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y = self.llm_model.generate(input_ids, max_new_tokens, prefix=features, temperature=temperature, top_k=top_k)\n",
    "\n",
    "        stop_tokens_ids = (y == 50256).nonzero()\n",
    "        end = len(y) if len(stop_tokens_ids)==1 else stop_tokens_ids[1]\n",
    "        ids_clean = y[1:end]\n",
    "        \n",
    "        pred = tokenizer.decode(ids_clean, skip_special_tokens=True)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SimpleEncoderConfig(Serializable):\n",
    "    # data params\n",
    "    block_size: int = 768\n",
    "    patch_size: int = 256\n",
    "\n",
    "    n_layers: int = 8\n",
    "    dim: int = 256\n",
    "    hidden_dim: int = 1024\n",
    "    n_registers: int = 16\n",
    "\n",
    "    head_dim: int = 32\n",
    "    n_heads: int = 16\n",
    "    n_kv_heads: int = 16 # now it should be the same with n_heads.\n",
    "    rope_theta: int = 10000\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SimpleMAEConfig(Serializable):\n",
    "    # data params\n",
    "    n_layers: int = 4\n",
    "    dim: int = 256\n",
    "    hidden_dim: int = 1024\n",
    "\n",
    "    head_dim: int = 32\n",
    "    n_heads: int = 8\n",
    "    n_kv_heads: int = 8 # now it should be the same with n_heads.\n",
    "    rope_theta: int = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleEncoderConfig(block_size=768, patch_size=256, n_layers=8, dim=256, hidden_dim=1024, n_registers=16, head_dim=32, n_heads=16, n_kv_heads=16, rope_theta=10000)\n",
      "Encoder: number of parameters: 10.76M\n",
      "MAE: number of parameters: 15.29M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-8:\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-5:\n",
      "Process ForkProcess-6:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "mae_weights = \"/drive/logs/kovalev/fixed_abs_mae_11M_5M_spikes/step_34000_loss_0.0166.safetensors\"\n",
    "\n",
    "mae_model = SimpleMAE( SimpleEncoderConfig(), SimpleMAEConfig())\n",
    "mae_model = torch.compile(mae_model)\n",
    "safetensors.torch.load_model(mae_model, mae_weights)\n",
    "mae_model = mae_model._orig_mod\n",
    "\n",
    "brain_model = mae_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n",
      "Full Franky: number of parameters: 135.42M\n",
      "Initing of the Franky completed\n",
      "Total: 136.01M, Trainable: 11.57M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(136007168, 11567360)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "dtype = torch.float32\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "llm_model = GPT.from_pretrained('gpt2', dict(dropout=0.0)) # \n",
    "\n",
    "### Create Franky model\n",
    "model = Franky(brain_model=brain_model, llm_model=llm_model)\n",
    "\n",
    "# weights = '/drive/logs/kovalev/fixed_franky_v2_spikes_only_projector/step_500_loss_3.4384.safetensors'\n",
    "# safetensors.torch.load_model(model, weights)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_proj\", \"c_attn\", 'c_fc','c_proj'])\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# for param in model.llm_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "for param in model.projector.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.date_embeddings.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.brain_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# for param in model.llm_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "print('Initing of the Franky completed')\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, Clip\n",
    "\n",
    "train_transform = Compose([\n",
    "    # AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.15, p=0.1),\n",
    "    Clip(0, 1, p=1)\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    Clip(0, 1, p=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = Path(\"/drive/data/competitionData\")\n",
    "# test_dataset = BrainDataset(data_path / 'test', tokenize_function=get_tokenizer(tokenizer), transform=train_transform)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# x, y, date = next(iter(test_dataloader))\n",
    "# print(x.shape, y.shape, date)\n",
    "# y = model(x, y, date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runed processing of the  /drive/data/competitionData/train\n",
      "len: 8800\n",
      "max input len 768\n",
      "Runed processing of the  /drive/data/competitionData/test\n",
      "len: 880\n",
      "max input len 768\n",
      "Launching training on one GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(split_batches=True)\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkoval_alvi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kovalev/frankenstein/notebooks_trainer/wandb/run-20240515_125133-tmhenk9d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/koval_alvi/frankenstein/runs/tmhenk9d' target=\"_blank\">all_tokens_train_all</a></strong> to <a href='https://wandb.ai/koval_alvi/frankenstein' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/koval_alvi/frankenstein' target=\"_blank\">https://wandb.ai/koval_alvi/frankenstein</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/koval_alvi/frankenstein/runs/tmhenk9d' target=\"_blank\">https://wandb.ai/koval_alvi/frankenstein/runs/tmhenk9d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device for training:  cuda\n",
      "Num devices:  1\n",
      "Completed initialization of scheduler\n",
      "****************************************************************************************************\n",
      "overall_steps 100: 3.347933053970337\n",
      "val loss: 3.3763537406921387\n",
      "saved model:  step_100_loss_3.3764.safetensors\n",
      "****************************************************************************************************\n",
      "overall_steps 200: 3.173790454864502\n",
      "val loss: 3.246567726135254\n",
      "saved model:  step_200_loss_3.2466.safetensors\n",
      "****************************************************************************************************\n",
      "overall_steps 300: 3.3049468994140625\n",
      "val loss: 3.1914420127868652\n",
      "saved model:  step_300_loss_3.1914.safetensors\n",
      "****************************************************************************************************\n",
      "overall_steps 400: 3.086965799331665\n",
      "val loss: 3.183224678039551\n",
      "saved model:  step_400_loss_3.1832.safetensors\n",
      "****************************************************************************************************\n",
      "overall_steps 500: 3.058110475540161\n",
      "val loss: 3.1539809703826904\n",
      "saved model:  step_500_loss_3.1540.safetensors\n",
      "****************************************************************************************************\n",
      "overall_steps 600: 3.1294898986816406\n",
      "val loss: 3.139838933944702\n",
      "saved model:  step_600_loss_3.1398.safetensors\n",
      "****************************************************************************************************\n",
      "overall_steps 700: 2.864269733428955\n",
      "val loss: 3.2026383876800537\n",
      "****************************************************************************************************\n",
      "overall_steps 800: 3.0980749130249023\n",
      "val loss: 3.117600679397583\n",
      "saved model:  step_800_loss_3.1176.safetensors\n",
      "****************************************************************************************************\n",
      "overall_steps 900: 2.9894700050354004\n",
      "val loss: 3.1605782508850098\n",
      "****************************************************************************************************\n",
      "overall_steps 1000: 3.376081705093384\n",
      "val loss: 3.148486375808716\n",
      "****************************************************************************************************\n",
      "overall_steps 1100: 3.163376808166504\n",
      "val loss: 3.135976552963257\n",
      "****************************************************************************************************\n",
      "overall_steps 1200: 2.748182773590088\n",
      "val loss: 3.177493095397949\n",
      "****************************************************************************************************\n",
      "overall_steps 1300: 2.8542299270629883\n",
      "val loss: 3.147087335586548\n",
      "****************************************************************************************************\n",
      "overall_steps 1400: 2.6738436222076416\n",
      "val loss: 3.202239513397217\n",
      "****************************************************************************************************\n",
      "overall_steps 1500: 2.9441890716552734\n",
      "val loss: 3.210588216781616\n",
      "****************************************************************************************************\n",
      "overall_steps 1600: 2.6302764415740967\n",
      "val loss: 3.193082094192505\n",
      "****************************************************************************************************\n",
      "overall_steps 1700: 2.6638216972351074\n",
      "val loss: 3.307210683822632\n",
      "****************************************************************************************************\n",
      "overall_steps 1800: 2.665591239929199\n",
      "val loss: 3.282745122909546\n",
      "****************************************************************************************************\n",
      "overall_steps 1900: 2.57297420501709\n",
      "val loss: 3.240435838699341\n",
      "****************************************************************************************************\n",
      "overall_steps 2000: 2.4912450313568115\n",
      "val loss: 3.4344749450683594\n",
      "****************************************************************************************************\n",
      "overall_steps 2100: 2.608159303665161\n",
      "val loss: 3.378244161605835\n",
      "****************************************************************************************************\n",
      "overall_steps 2200: 2.6337828636169434\n",
      "val loss: 3.3098866939544678\n",
      "****************************************************************************************************\n",
      "overall_steps 2300: 2.5786614418029785\n",
      "val loss: 3.5216877460479736\n",
      "****************************************************************************************************\n",
      "overall_steps 2400: 2.4140689373016357\n",
      "val loss: 3.418879985809326\n",
      "****************************************************************************************************\n",
      "overall_steps 2500: 2.1292154788970947\n",
      "val loss: 3.598048210144043\n",
      "****************************************************************************************************\n",
      "overall_steps 2600: 2.1707651615142822\n",
      "val loss: 3.6705193519592285\n",
      "****************************************************************************************************\n",
      "overall_steps 2700: 2.2780473232269287\n",
      "val loss: 3.560480833053589\n",
      "****************************************************************************************************\n",
      "overall_steps 2800: 1.998302698135376\n",
      "val loss: 3.707714796066284\n",
      "****************************************************************************************************\n",
      "overall_steps 2900: 2.0533618927001953\n",
      "val loss: 3.7950081825256348\n",
      "****************************************************************************************************\n",
      "overall_steps 3000: 2.1547088623046875\n",
      "val loss: 3.687105417251587\n",
      "****************************************************************************************************\n",
      "overall_steps 3100: 1.8972669839859009\n",
      "val loss: 3.81296443939209\n",
      "****************************************************************************************************\n",
      "overall_steps 3200: 2.233091354370117\n",
      "val loss: 3.7582390308380127\n",
      "****************************************************************************************************\n",
      "overall_steps 3300: 2.0363259315490723\n",
      "val loss: 3.746509075164795\n",
      "****************************************************************************************************\n",
      "overall_steps 3400: 1.9618722200393677\n",
      "val loss: 3.942349910736084\n",
      "****************************************************************************************************\n",
      "overall_steps 3500: 1.8389530181884766\n",
      "val loss: 3.8512749671936035\n",
      "****************************************************************************************************\n",
      "overall_steps 3600: 1.6183011531829834\n",
      "val loss: 4.01300573348999\n",
      "****************************************************************************************************\n",
      "overall_steps 3700: 1.735843300819397\n",
      "val loss: 3.9595470428466797\n",
      "****************************************************************************************************\n",
      "overall_steps 3800: 1.778174877166748\n",
      "val loss: 3.926265239715576\n",
      "****************************************************************************************************\n",
      "overall_steps 3900: 1.6960678100585938\n",
      "val loss: 4.003187656402588\n",
      "****************************************************************************************************\n",
      "overall_steps 4000: 1.8794169425964355\n",
      "val loss: 4.027645111083984\n",
      "****************************************************************************************************\n",
      "overall_steps 4100: 1.9907091856002808\n",
      "val loss: 3.945430278778076\n",
      "****************************************************************************************************\n",
      "overall_steps 4200: 1.6029343605041504\n",
      "val loss: 4.068284511566162\n",
      "****************************************************************************************************\n",
      "overall_steps 4300: 1.772065281867981\n",
      "val loss: 3.9781665802001953\n",
      "****************************************************************************************************\n",
      "overall_steps 4400: 1.600696325302124\n",
      "val loss: 4.022862911224365\n",
      "****************************************************************************************************\n",
      "overall_steps 4500: 1.453879475593567\n",
      "val loss: 4.1105780601501465\n",
      "****************************************************************************************************\n",
      "overall_steps 4600: 1.7437728643417358\n",
      "val loss: 4.147835731506348\n",
      "****************************************************************************************************\n",
      "overall_steps 4700: 1.507262945175171\n",
      "val loss: 4.1413726806640625\n",
      "****************************************************************************************************\n",
      "overall_steps 4800: 1.3746522665023804\n",
      "val loss: 4.1563029289245605\n",
      "****************************************************************************************************\n",
      "overall_steps 4900: 1.4963583946228027\n",
      "val loss: 4.108671188354492\n",
      "****************************************************************************************************\n",
      "overall_steps 5000: 1.4854892492294312\n",
      "val loss: 4.217744827270508\n",
      "****************************************************************************************************\n",
      "overall_steps 5100: 1.4535855054855347\n",
      "val loss: 4.248703479766846\n",
      "****************************************************************************************************\n",
      "overall_steps 5200: 1.5060187578201294\n",
      "val loss: 4.196557998657227\n",
      "****************************************************************************************************\n",
      "overall_steps 5300: 1.383873462677002\n",
      "val loss: 4.222593784332275\n",
      "****************************************************************************************************\n",
      "overall_steps 5400: 1.3896251916885376\n",
      "val loss: 4.128073215484619\n",
      "****************************************************************************************************\n",
      "overall_steps 5500: 1.4140636920928955\n",
      "val loss: 4.168605327606201\n",
      "****************************************************************************************************\n",
      "overall_steps 5600: 1.3819242715835571\n",
      "val loss: 4.236654758453369\n",
      "****************************************************************************************************\n",
      "overall_steps 5700: 1.4175362586975098\n",
      "val loss: 4.199875831604004\n",
      "******************************************************"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# indices = torch.arange(32).repeat(4)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# train_dataset = torch.utils.data.Subset(train_dataset, indices)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# test_dataset = torch.utils.data.Subset(test_dataset, indices)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m args \u001b[38;5;241m=\u001b[39m (model, (train_dataset, test_dataset), train_config, project_name, save_folder)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_train_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# simple_train_model(*args)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/launchers.py:222\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching training on CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 222\u001b[0m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/kovalev/frankenstein/utils/train_utils.py:140\u001b[0m, in \u001b[0;36mrun_train_model\u001b[0;34m(model, datasets, config, project_name, save_folder)\u001b[0m\n\u001b[1;32m    137\u001b[0m inputs, labels, date_info \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    139\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m model(inputs, labels, date_info\u001b[38;5;241m=\u001b[39mdate_info)\n\u001b[0;32m--> 140\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39msync_gradients:\n\u001b[1;32m    143\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mgrad_clip)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py:2120\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2120\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "project_name = 'frankenstein'\n",
    "\n",
    "train_config = TrainConfig(exp_name='all_tokens_train_all',\n",
    "                           mixed_precision=True, \n",
    "                           batch_size=32, \n",
    "                           num_workers=3, \n",
    "                           pin_memory=True, \n",
    "                           warmup_iters=1000,\n",
    "                           eval_interval=100,\n",
    "                           grad_accum=1, \n",
    "                           weight_decay=1e-5)\n",
    "# peter path\n",
    "# data_path = Path(r'C:\\Users\\peter\\alvi\\brain2text\\competitionData')\n",
    "data_path = Path(\"/drive/data/competitionData\")\n",
    "save_folder = Path(\"/drive/logs/kovalev\")\n",
    "\n",
    "train_dataset = BrainDataset(data_path / 'train', tokenize_function=get_tokenizer(tokenizer), transform=train_transform)\n",
    "test_dataset = BrainDataset(data_path / 'test', tokenize_function=get_tokenizer(tokenizer), transform=test_transform)\n",
    "\n",
    "# indices = torch.arange(32).repeat(4)\n",
    "# train_dataset = torch.utils.data.Subset(train_dataset, indices)\n",
    "# test_dataset = torch.utils.data.Subset(test_dataset, indices)\n",
    "args = (model, (train_dataset, test_dataset), train_config, project_name, save_folder)\n",
    "notebook_launcher(run_train_model, args, num_processes=1)\n",
    "\n",
    "# simple_train_model(*args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 10\n",
    "\n",
    "for i, sample in enumerate(train_dataset):\n",
    "    \n",
    "    x, gt, _ = sample\n",
    "    \n",
    "    pred = model.generate(x, tokenizer=tokenizer)\n",
    "    \n",
    "    gt[gt==-100]=50256\n",
    "    gt_txt = tokenizer.decode(gt, skip_special_tokens=True)\n",
    "    \n",
    "    print('pred: ', pred)\n",
    "    print('gt_txt: ', gt_txt)\n",
    "    print('----')\n",
    "    if i > max:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, pred = tokenizer.decode(ids_clean, skip_special_tokens=False)\n",
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(x, tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
