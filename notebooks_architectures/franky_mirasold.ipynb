{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from simple_parsing.helpers import Serializable\n",
    "import time\n",
    "import numpy as np\n",
    "## Functions\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.bert import BrainBert, BertConfig\n",
    "from models.vq_brain_per_channel import SoundStream, VAEConfig\n",
    "from models.blocks import Block, build_complex_rope_cache, RMSNorm, build_advanced_causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Franky(nn.Module): \n",
    "    \"\"\"This is first model which incorporate brain features into LLM\"\"\"\n",
    "\n",
    "    def __init__(self, combiner_config, causal_config, brain_model, llm_model, tokenizer=None):\n",
    "        super().__init__()\n",
    "        self.brain_model = brain_model\n",
    "        self.llm_model = llm_model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "\n",
    "        self.combiner_config = combiner_config\n",
    "        self.causal_config = causal_config\n",
    "\n",
    "        self.n_electrodes = brain_model.config.n_electrodes\n",
    "        self.window_size = self.brain_model.config.window_size\n",
    "        self.block_size = int(self.window_size / self.brain_model.tokenizer.downsample * self.combiner_config.n_registers)\n",
    "        self.dim = combiner_config.dim\n",
    "        \n",
    "        \n",
    "        self.combiner_model = nn.Sequential(*[Block(combiner_config) for _ in range(combiner_config.n_layers)])\n",
    "        \n",
    "        self.combiner_pos_embeddings = nn.Parameter(torch.randn(1, self.n_electrodes, combiner_config.dim))\n",
    "        \n",
    "        # Causal model\n",
    "        # init new rope cache for working with several registers and overwriting old one\n",
    "        # we have to repeat values, because n_registers have same time step\n",
    "        causal_config.block_size = self.block_size\n",
    "        \n",
    "        self.causal_model = CausalModel(causal_config)\n",
    "        self.causal_model.attn_mask = build_advanced_causal_mask(self.block_size, self.combiner_config.n_registers)\n",
    "        old_rope = self.causal_model.precompute_rope_cash\n",
    "        self.causal_model.precompute_rope_cash = old_rope.repeat_interleave(self.combiner_config.n_registers, dim=0)\n",
    "\n",
    "        \n",
    "        self.projector = nn.Linear(brain_model.config.dim, llm_model.config.n_embd)\n",
    "        \n",
    "\n",
    "        self.date_embeddings = nn.Embedding(num_embeddings=25, embedding_dim=llm_model.config.n_embd)\n",
    "        \n",
    "        print(\"Full Franky: number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "\n",
    "    def combine_features(self, x):\n",
    "        \"\"\"\n",
    "        Combine features from different channels into several vectors.\n",
    "        x: b, t, c, d\n",
    "        \"\"\"\n",
    "        print(x.shape)\n",
    "        b, t, c, d = x.size()\n",
    "        x = rearrange(x, 'b t c d -> (b t) c d', b=b, t=t, c=self.n_electrodes, d=self.dim)\n",
    "\n",
    "        x = x + self.combiner_pos_embeddings\n",
    "        tokens = self.combiner_model(x)\n",
    "\n",
    "        tokens = tokens[:, :self.combiner_config.n_registers]\n",
    "        tokens = rearrange(tokens, '(b t) c d -> b (c t) d', b=b, t=t, c=self.combiner_config.n_registers, d=self.brain_model.dim)\n",
    "        return tokens \n",
    "    \n",
    "    \n",
    "    def forward(self, x, targets=None, date_info=None):\n",
    "        \"\"\"\n",
    "        Train model.\n",
    "        x: B, T, C\n",
    "        \"\"\"\n",
    "        is_padded = (x==0).all(dim=-1) # B, T\n",
    "        is_padded = is_padded[:, ::4]\n",
    "\n",
    "        _, x = self.brain_model(x) # b, t, c, d\n",
    "\n",
    "        x = self.combine_features(x)\n",
    "        pred_latent = self.causal_model(x)\n",
    "\n",
    "        # Also we have to add padded tokens here. and do not calculate metrics on them.\n",
    "        # future_loss = F.mse_loss(pred_latent[:, :-self.combiner_config.n_registers], x[:, :-self.combiner_config.n_registers])\n",
    "        \n",
    "        features = self.projector(x)\n",
    "        print('brain features shape', features.shape)\n",
    "        print('is_padded', is_padded.shape)\n",
    "        \n",
    "\n",
    "        # date_embedding = self.date_embeddings(date_info)\n",
    "        # x = torch.cat([x, date_embedding], dim=-1)\n",
    "        \n",
    "        new_idx = targets.clone()\n",
    "        new_idx[new_idx == -100] = self.tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "        outputs = self.llm_model(input_ids=new_idx[:, :-1], \n",
    "                                 labels=targets[:, 1:], \n",
    "                                 encoder_hidden_states=features, \n",
    "                                 encoder_attention_mask=~is_padded)\n",
    "        \n",
    "        return outputs.loss, outputs.logits\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, x, date_info=None, tokenizer=None):\n",
    "        device = self.device\n",
    "        \n",
    "        x = torch.from_numpy(x[None, ]).to(device).to(self.dtype)\n",
    "        features = self.brain_model(x)\n",
    "        \n",
    "        ### Text part\n",
    "        start = tokenizer.bos_token\n",
    "        input_ids = tokenizer(start,  return_tensors=\"pt\")['input_ids'].to(self.device)\n",
    "\n",
    "        # max_new_tokens = 25\n",
    "        # temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "        # top_k = 10\n",
    "\n",
    "        res = self.llm_model.generate(input_ids=input_ids, encoder_hidden_states=features)\n",
    "        pred = self.tokenizer.batch_decode(res)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "\n",
    "    def get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "    \n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return next(self.parameters()).dtype\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.codebook_size 1920\n",
      "self.downsample 8\n"
     ]
    }
   ],
   "source": [
    "vae_config = VAEConfig(C=256, levels=(8, 8, 6, 5))\n",
    "vq_vae = SoundStream(**vae_config.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig(window_size=128, n_electrodes=256, mask_ratio=0, tokenizer_downsample=8, n_layers=12, dim=64, hidden_dim=1024, head_dim=32, n_heads=12, n_kv_heads=12)\n",
      "Encoder: number of parameters: 20.74M\n"
     ]
    }
   ],
   "source": [
    "bert_config = BertConfig(dim=64, \n",
    "                        window_size=window_size, \n",
    "                        tokenizer_downsample=int(vq_vae.downsample),\n",
    "                        n_electrodes=256, \n",
    "                        mask_ratio=0, \n",
    "                        n_layers=12, \n",
    "                        n_heads=12, \n",
    "                        n_kv_heads=12)\n",
    "\n",
    "bert = BrainBert(bert_config, vq_vae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CombinerConfig(Serializable):\n",
    "    # data params\n",
    "    n_registers: int = 2\n",
    "    n_layers: int = 4\n",
    "    dim: int = 64\n",
    "    hidden_dim: int = 1024\n",
    "\n",
    "    head_dim: int = 32\n",
    "    n_heads: int = 16\n",
    "    n_kv_heads: int = 16 \n",
    "\n",
    "@dataclass\n",
    "class CausalModelConfig(Serializable):\n",
    "\n",
    "    block_size: int = 0\n",
    "    rope_theta: float = 1000.0\n",
    "\n",
    "    # data params\n",
    "    n_layers: int = 4\n",
    "    dim: int = 64\n",
    "    hidden_dim: int = 1024\n",
    "    dropout: float = 0.0\n",
    "    \n",
    "\n",
    "    head_dim: int = 32\n",
    "    n_heads: int = 16\n",
    "    n_kv_heads: int = 16 \n",
    "    calculate_loss: bool = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.3.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.bias', 'h.0.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.bias', 'h.0.crossattention.q_attn.bias', 'h.7.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.3.crossattention.q_attn.weight', 'h.2.crossattention.q_attn.weight', 'h.8.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.weight', 'h.10.ln_cross_attn.weight', 'h.5.crossattention.c_proj.weight', 'h.2.ln_cross_attn.bias', 'h.4.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.10.crossattention.c_proj.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.q_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.9.ln_cross_attn.bias', 'h.0.ln_cross_attn.bias', 'h.3.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.c_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.10.ln_cross_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.4.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.7.ln_cross_attn.bias', 'h.8.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.5.ln_cross_attn.weight', 'h.2.crossattention.q_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.bias', 'h.5.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.c_attn.bias', 'h.11.ln_cross_attn.bias', 'h.8.crossattention.c_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.1.ln_cross_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.crossattention.c_attn.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.bias', 'h.11.ln_cross_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.1.crossattention.c_proj.weight', 'h.10.crossattention.c_attn.bias', 'h.3.ln_cross_attn.weight', 'h.10.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.8.ln_cross_attn.weight', 'h.11.crossattention.q_attn.weight', 'h.9.ln_cross_attn.weight', 'h.9.crossattention.q_attn.bias', 'h.4.ln_cross_attn.weight', 'h.6.crossattention.q_attn.bias', 'h.3.crossattention.c_proj.weight', 'h.2.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.7.crossattention.c_attn.weight', 'h.6.ln_cross_attn.bias', 'h.7.crossattention.c_proj.bias', 'h.9.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_attn.bias', 'h.2.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.1.crossattention.q_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.c_proj.weight', 'h.3.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.2.crossattention.c_attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained('gpt2', add_cross_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the rope cache:  torch.Size([32, 16])\n",
      "Full Franky: number of parameters: 176.26M\n"
     ]
    }
   ],
   "source": [
    "combiner_config = CombinerConfig()\n",
    "causal_config = CausalModelConfig()\n",
    "\n",
    "\n",
    "model = Franky(combiner_config, causal_config, bert, gpt2, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 256, 64])\n",
      "brain features shape torch.Size([1, 32, 768])\n",
      "is_padded torch.Size([1, 32])\n",
      "torch.Size([1, 24, 50257])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 128, 256)\n",
    "targers = torch.arange(25).unsqueeze(0)\n",
    "\n",
    "loss, features = model(x, targers)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "window_size = 32\n",
    "n_electrodes = 256\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    \n",
    "    # A.CoarseDropout(fill_value=0, p=0.5),\n",
    "    # A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=0.5),\n",
    "    # A.GaussNoise(var_limit=0.005, mean=0, p=0.5),\n",
    "\n",
    "    A.PadIfNeeded(min_height=window_size, min_width=n_electrodes, position='top_left', \n",
    "                  border_mode=0, value=0, always_apply=True),\n",
    "    # A.RandomCrop(height=window_size, width=n_electrodes, always_apply=True),\n",
    "    A.Crop(x_min=0, x_max=n_electrodes, y_min=0, y_max=window_size, always_apply=True),\n",
    "\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.PadIfNeeded(min_height=window_size, min_width=n_electrodes, position='top_left', \n",
    "                  border_mode=0, value=0, always_apply=True),\n",
    "    A.Crop(x_min=0, x_max=n_electrodes, y_min=0, y_max=window_size, always_apply=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
