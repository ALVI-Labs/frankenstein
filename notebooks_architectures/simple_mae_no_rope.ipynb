{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03df07bb-a652-4a5e-a204-cc0d29190bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from simple_parsing.helpers import Serializable\n",
    "import time\n",
    "import numpy as np\n",
    "## Functions\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w1 = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(config.hidden_dim, config.dim, bias=False)\n",
    "        self.w3 = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "class CausalSelfAttention(nn.Module): \n",
    "    \"\"\"\n",
    "    Simple Multi head attention with einops and F.scaled_dot_product_attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, is_causal=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert config.n_heads == config.n_kv_heads, \"n_heads should be equal n_kv_heads\"\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "        self.n_kv_heads = config.n_heads # for simplicity we use vanilla transformer.\n",
    "        self.repeats = self.n_heads // self.n_kv_heads\n",
    "        self.head_dim = config.head_dim\n",
    "\n",
    "        self.qw = nn.Linear(config.dim, config.head_dim * config.n_heads, bias=False)\n",
    "        self.kw = nn.Linear(config.dim, config.head_dim * config.n_kv_heads, bias=False)\n",
    "        self.vw = nn.Linear(config.dim, config.head_dim * config.n_kv_heads, bias=False)\n",
    "\n",
    "        self.project = nn.Linear(config.head_dim * config.n_heads, config.dim, bias=False)\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        B, T, C = x.size() # b, t, c*h        \n",
    "        q, k, v = self.qw(x), self.kw(x), self.vw(x) \n",
    "\n",
    "        # split by n_heads.\n",
    "        q = rearrange(q, 'b t (nh c) -> b t nh c', b=B, t=T, nh=self.n_heads, c=self.head_dim)\n",
    "        k = rearrange(k, 'b t (nh c) -> b t nh c', b=B, t=T, nh=self.n_heads, c=self.head_dim)\n",
    "        v = rearrange(v, 'b t (nh c) -> b t nh c', b=B, t=T, nh=self.n_heads, c=self.head_dim)\n",
    "\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            t_q, t_k = q.size(1), k.size(1)\n",
    "            attn_mask = attn_mask[..., :t_q, :t_k]\n",
    "\n",
    "        q = q.transpose(1, 2)  # (B, nh, T, c)\n",
    "        k = k.transpose(1, 2)  # (B, nh, T, c)\n",
    "        v = v.transpose(1, 2)  # (B, nh, T, c)\n",
    "        \n",
    "        res = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)  \n",
    "\n",
    "        res = rearrange(res, 'b nh t c -> b t (nh c)', b=B, t=T, nh=self.n_heads, c=self.head_dim)\n",
    "        res = self.project(res)\n",
    "\n",
    "        return res\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = RMSNorm(config.dim)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = RMSNorm(config.dim)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        x = x + self.attn(self.ln_1(x), attn_mask) \n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            emb = nn.Linear(config.patch_size, config.dim),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layers)]),\n",
    "            ln_f = nn.LayerNorm(config.dim),\n",
    "        ))\n",
    "        num_patches = config.block_size\n",
    "        self.n_registers = config.n_registers\n",
    "        self.registers = nn.Parameter(torch.zeros(1, self.n_registers, config.dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + self.n_registers, config.dim))\n",
    "\n",
    "        self.attn_mask = torch.ones(config.block_size+self.n_registers, config.block_size+self.n_registers).to(torch.bool)\n",
    "        \n",
    "        print(config)\n",
    "        print(\"Encoder: number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_attn_mask_padded(self, x):\n",
    "        is_padded = (x == 0).all(dim=2)\n",
    "        # is_padded = torch.ones(self.n_registers)is_padded\n",
    "        attn_mask = ~is_padded.unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        attn_mask = F.pad(attn_mask, (self.n_registers, 0, self.n_registers, 0), 'constant', value=True)\n",
    "        return attn_mask\n",
    "        \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        myo signals with shape: with shape [B, T, C]\n",
    "        \"\"\"\n",
    "        attn_mask = self.attn_mask if attn_mask is None else attn_mask\n",
    "        attn_mask = attn_mask.to(self.device)\n",
    "\n",
    "        \n",
    "\n",
    "        b, t, c = x.shape\n",
    "        x = self.transformer.emb(x)\n",
    "        \n",
    "        registers = self.registers.expand(b, -1, -1)\n",
    "        x = torch.cat([registers, x], axis=1)\n",
    "        \n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x, attn_mask=attn_mask)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return next(self.parameters()).dtype\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def rope_cache(self) -> torch.Tensor:\n",
    "        # Just to use proper device.\n",
    "        if self.precompute_rope_cash.device != self.device:\n",
    "            self.precompute_rope_cash = self.precompute_rope_cash.to(device=self.device)\n",
    "        return self.precompute_rope_cash\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "\n",
    "\n",
    "class SimpleMAE(nn.Module):\n",
    "    def __init__(self, encoder_config, mae_config):\n",
    "        super().__init__()\n",
    "        self.encoder_config = encoder_config\n",
    "        \n",
    "        self.encoder = SimpleEncoder(encoder_config)\n",
    "        self.dim = mae_config.dim\n",
    "\n",
    "        self.decoder = nn.ModuleDict(dict(\n",
    "            emb = nn.Linear(encoder_config.dim, mae_config.dim), # connection between them.\n",
    "            h = nn.ModuleList([Block(mae_config) for _ in range(mae_config.n_layers)]),\n",
    "        ))\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.randn(mae_config.dim))\n",
    "        self.decoder_pos_emb = nn.Parameter(torch.randn(1, encoder_config.block_size + encoder_config.n_registers, mae_config.dim))\n",
    "        self.to_signals = nn.Linear(mae_config.dim, encoder_config.patch_size)\n",
    "\n",
    "        print(\"MAE: number of parameters: %.2fM\" % (self.get_num_params()/1e6))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "    def get_masking_indices(self, masking_ratio, x):\n",
    "        b, n_tokens, _ = x.shape\n",
    "\n",
    "        num_masked = int(masking_ratio * n_tokens)\n",
    "        rand_indices = torch.rand(b, n_tokens, device = x.device).argsort(dim=-1) # get idxs of random values.\n",
    "        masked_indices, unmasked_indices = rand_indices[:, :num_masked], rand_indices[:, num_masked:]\n",
    "\n",
    "        masked_indices, _ = torch.sort(masked_indices, dim=1)\n",
    "        unmasked_indices, _ = torch.sort(unmasked_indices, dim=1)      \n",
    "        \n",
    "        return masked_indices, unmasked_indices\n",
    "    \n",
    "\n",
    "    def forward(self, x, targets=None, date_info=None, masking_ratio=0.75, return_preds=False):\n",
    "        \"\"\"\n",
    "        Inputs: x with shape -> B, T, C\n",
    "        \"\"\"\n",
    "        b, t, c = x.shape\n",
    "\n",
    "        # Attention mask for padded tokens.\n",
    "        registers = self.encoder.registers.expand(b, -1, -1)\n",
    "        n_registers = registers.size(1)\n",
    "        \n",
    "        is_padded = (x == 0).all(dim=2)\n",
    "        attn_mask = ~is_padded.unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        \n",
    "        # Masking\n",
    "        masked_indices, unmasked_indices = self.get_masking_indices(masking_ratio, x) # [b, N]\n",
    "        batch_range = torch.arange(b, device=x.device)[:, None]\n",
    "\n",
    "        attn_mask_unmasked = attn_mask[batch_range[..., None], unmasked_indices[..., None], unmasked_indices[:, None, :]]\n",
    "        attn_mask_unmasked = F.pad(attn_mask_unmasked, (n_registers, 0, n_registers, 0), 'constant', value=True)\n",
    "        attn_mask_unmasked = rearrange(attn_mask_unmasked, 'b h w -> b 1 h w')\n",
    "        \n",
    "        # Encoder\n",
    "        registers = registers + self.encoder.pos_embedding[:, :n_registers]\n",
    "        \n",
    "        tokens = self.encoder.transformer.emb(x)\n",
    "        tokens = tokens + self.encoder.pos_embedding[:, n_registers:n_registers+t,:]\n",
    "        tokens = tokens[batch_range, unmasked_indices]\n",
    "        \n",
    "        tokens = torch.cat([registers, tokens], axis=1)\n",
    "\n",
    "        for block in self.encoder.transformer.h:\n",
    "            tokens = block(tokens, attn_mask_unmasked)\n",
    "        \n",
    "        ### DECODER \n",
    "        attn_mask = rearrange(attn_mask, 'b h w -> b 1 h w') # include registers. \n",
    "\n",
    "        unmasked_decoder_tokens = self.decoder.emb(tokens)\n",
    "\n",
    "        decoder_tokens = torch.zeros(b, t, self.dim, device=x.device, dtype=x.dtype)\n",
    "        decoder_tokens[batch_range, unmasked_indices] = unmasked_decoder_tokens[:, n_registers:]\n",
    "        decoder_tokens[batch_range, masked_indices] = self.mask_token\n",
    "\n",
    "        decoder_tokens = torch.cat([unmasked_decoder_tokens[:, :n_registers], decoder_tokens], axis=1)\n",
    "        decoder_tokens = decoder_tokens + self.decoder_pos_emb[:, :n_registers+t]\n",
    "\n",
    "        attn_mask = F.pad(attn_mask, (n_registers, 0, n_registers, 0), 'constant', value=True)\n",
    "\n",
    "        for block in self.decoder.h:\n",
    "            decoder_tokens = block(decoder_tokens, attn_mask)\n",
    "        \n",
    "        decoder_tokens = decoder_tokens[:, n_registers:]\n",
    "        \n",
    "        pred_tokens = self.to_signals(decoder_tokens)\n",
    "\n",
    "        ### LOSS: mse on masked and not padded tokens.\n",
    "        tokens_pred_masked = pred_tokens[batch_range, masked_indices]\n",
    "        tokens_real_masked = x[batch_range, masked_indices]\n",
    "\n",
    "        # let's calculate loss on masked and not padded tokens.\n",
    "        mask_valid = ~is_padded[batch_range, masked_indices]\n",
    "        loss_tensor = F.mse_loss(tokens_pred_masked, tokens_real_masked, reduction='none')\n",
    "        \n",
    "        loss_real_values = loss_tensor[mask_valid.nonzero(as_tuple=True)]\n",
    "        recon_loss = torch.mean(loss_real_values)\n",
    "            \n",
    "        if return_preds:\n",
    "            binary_mask = torch.zeros_like(x, device=x.device, dtype=x.dtype) \n",
    "            binary_mask[batch_range, masked_indices] = 1\n",
    "\n",
    "            reconstruction_signal = torch.zeros_like(x, device=x.device, dtype=x.dtype)\n",
    "            reconstruction_signal[batch_range, masked_indices] = tokens_pred_masked\n",
    "            reconstruction_signal[batch_range, unmasked_indices] = x[batch_range, unmasked_indices]\n",
    "\n",
    "            return recon_loss, reconstruction_signal, binary_mask\n",
    "\n",
    "        return recon_loss, None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abab5468-b245-4d8e-8009-c3cf8d084c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SimpleEncoderConfig(Serializable):\n",
    "    # data params\n",
    "    block_size: int = 768\n",
    "    patch_size: int = 256\n",
    "\n",
    "    n_layers: int = 8\n",
    "    dim: int = 256\n",
    "    hidden_dim: int = 1024\n",
    "    n_registers: int = 4\n",
    "\n",
    "    head_dim: int = 32\n",
    "    n_heads: int = 16\n",
    "    n_kv_heads: int = 16 # now it should be the same with n_heads.\n",
    "    rope_theta: int = 10000\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SimpleMAEConfig(Serializable):\n",
    "    # data params\n",
    "    n_layers: int = 4\n",
    "    dim: int = 256\n",
    "    hidden_dim: int = 1024\n",
    "\n",
    "    head_dim: int = 32\n",
    "    n_heads: int = 8\n",
    "    n_kv_heads: int = 8 # now it should be the same with n_heads.\n",
    "    rope_theta: int = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a07e10-fd1d-4c55-a382-adb51c5e653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleEncoderConfig(block_size=768, patch_size=256, n_layers=8, dim=256, hidden_dim=1024, n_registers=4, head_dim=32, n_heads=16, n_kv_heads=16, rope_theta=10000)\n",
      "Encoder: number of parameters: 10.75M\n",
      "SimpleEncoderConfig(block_size=768, patch_size=256, n_layers=8, dim=256, hidden_dim=1024, n_registers=4, head_dim=32, n_heads=16, n_kv_heads=16, rope_theta=10000)\n",
      "Encoder: number of parameters: 10.75M\n",
      "MAE: number of parameters: 15.28M\n"
     ]
    }
   ],
   "source": [
    "cfg=SimpleEncoderConfig()\n",
    "model = SimpleEncoder(cfg)\n",
    "model = SimpleMAE(SimpleEncoderConfig(), SimpleMAEConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95e2d3b-6bcd-455a-b65a-40134d5252da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.6761, grad_fn=<MeanBackward0>), None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(15, 500, 256)\n",
    "y = model(x, masking_ratio=0.25)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c7bae1-6789-4f7f-a6a1-dea5e0705558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44d08ea5-4671-4389-ba7b-e643d4090bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f52f0e38d30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = torch.zeros(15, 192, 192).to(torch.bool)\n",
    "res = F.pad(m, (4,0, 4,0), 'constant', value=True)\n",
    "plt.imshow(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456285f7-5bfb-4797-b8e2-b5126f115e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d011d53a-44ec-49c0-8ba5-cc93228edee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc0c4f-0fe1-418e-8b7a-1b18d4447473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28301d-2c0c-43be-a26b-c6061505cab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac05642-439c-4842-8337-095af9c5d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import time\n",
    "\n",
    "# class MaskCreator:\n",
    "#     def __init__(self, n_registers):\n",
    "#         self.n_registers = n_registers\n",
    "\n",
    "#     def get_attn_mask_padded(self, x):\n",
    "#         seq_len = x.size(1)\n",
    "#         is_padded = (x == 0).all(dim=2)\n",
    "#         attn_mask = ~is_padded  # Negate first\n",
    "\n",
    "#         # Create an initial mask with registers considered not padded\n",
    "#         full_mask = torch.ones((b, seq_len + self.n_registers, seq_len + self.n_registers), dtype=torch.bool)\n",
    "\n",
    "#         # Set the non-register parts\n",
    "#         full_mask[:, self.n_registers:, self.n_registers:] = attn_mask.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\n",
    "#         return full_mask\n",
    "\n",
    "# # Set dimensions and parameters\n",
    "# n_registers = 10\n",
    "# b, n_tokens, dim = 128, 512, 64  # Example dimensions for a large batch with sequence length\n",
    "# x = torch.randn(b, n_tokens, dim)\n",
    "# x[:, 400:] = 0  # Introducing some padding\n",
    "\n",
    "# # Create an instance of the class\n",
    "# mask_creator = MaskCreator(n_registers=n_registers)\n",
    "\n",
    "# # Time the function\n",
    "# start_time = time.time()\n",
    "# mask = mask_creator.get_attn_mask_padded(x)\n",
    "# elapsed_time = time.time() - start_time\n",
    "# elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3da699-695f-4f00-8ea4-51a1851c123f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
