{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "import safetensors\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import notebook_launcher\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from simple_parsing import ArgumentParser\n",
    "import einops\n",
    "\n",
    "from models import brainformer\n",
    "from utils.data_utils import BrainDataset, get_tokenizer\n",
    "from utils.train_utils import TrainConfig, run_train_model, count_parameters\n",
    "\n",
    "from torch import nn\n",
    "from models.brainformer import Encoder, CrossBlock, build_complex_rope_cache, Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from models.gpt2_model import GPT\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from contextlib import nullcontext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "dtype = 'float32'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n",
      "initing completed\n",
      "tensor([0., 0.], device='cuda:0')\n",
      "Starting generation\n",
      "beams.shape, prefix.shape torch.Size([2, 1]) torch.Size([2, 16, 768])\n",
      "-----\n",
      "new token\n",
      "logits.shape torch.Size([2, 50257])\n",
      "[(tensor(-1.7620, device='cuda:0'), 0, tensor(464, device='cuda:0')), (tensor(-1.7620, device='cuda:0'), 1, tensor(464, device='cuda:0')), (tensor(-2.9174, device='cuda:0'), 0, tensor(32, device='cuda:0')), (tensor(-2.9174, device='cuda:0'), 1, tensor(32, device='cuda:0'))]\n",
      "beam_scores tensor([-1.7620, -1.7620], device='cuda:0')\n",
      "-----\n",
      "new token\n",
      "logits.shape torch.Size([2, 50257])\n",
      "[(tensor(-5.4096, device='cuda:0'), 0, tensor(366, device='cuda:0')), (tensor(-5.4096, device='cuda:0'), 1, tensor(366, device='cuda:0')), (tensor(-5.4636, device='cuda:0'), 0, tensor(717, device='cuda:0')), (tensor(-5.4636, device='cuda:0'), 1, tensor(717, device='cuda:0'))]\n",
      "beam_scores tensor([-5.4096, -5.4096], device='cuda:0')\n",
      "-----\n",
      "new token\n",
      "logits.shape torch.Size([2, 50257])\n",
      "[(tensor(-7.1713, device='cuda:0'), 0, tensor(198, device='cuda:0')), (tensor(-7.1713, device='cuda:0'), 1, tensor(198, device='cuda:0')), (tensor(-8.4674, device='cuda:0'), 0, tensor(464, device='cuda:0')), (tensor(-8.4674, device='cuda:0'), 1, tensor(464, device='cuda:0'))]\n",
      "beam_scores tensor([-7.1713, -7.1713], device='cuda:0')\n",
      "-----\n",
      "new token\n",
      "logits.shape torch.Size([2, 50257])\n",
      "[(tensor(-7.8270, device='cuda:0'), 0, tensor(198, device='cuda:0')), (tensor(-7.8270, device='cuda:0'), 1, tensor(198, device='cuda:0')), (tensor(-10.3214, device='cuda:0'), 0, tensor(366, device='cuda:0')), (tensor(-10.3214, device='cuda:0'), 1, tensor(366, device='cuda:0'))]\n",
      "beam_scores tensor([-7.8270, -7.8270], device='cuda:0')\n",
      "-----\n",
      "new token\n",
      "logits.shape torch.Size([2, 50257])\n",
      "[(tensor(-8.4804, device='cuda:0'), 0, tensor(198, device='cuda:0')), (tensor(-8.4804, device='cuda:0'), 1, tensor(198, device='cuda:0')), (tensor(-10.9629, device='cuda:0'), 0, tensor(366, device='cuda:0')), (tensor(-10.9629, device='cuda:0'), 1, tensor(366, device='cuda:0'))]\n",
      "beam_scores tensor([-8.4804, -8.4804], device='cuda:0')\n",
      "<|endoftext|>\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT.from_pretrained('gpt2', dict(dropout=0.0))\n",
    "model.eval().to(device)\n",
    "\n",
    "print('initing completed')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start = '<|endoftext|>'\n",
    "input_ids = tokenizer(start,  return_tensors=\"pt\")['input_ids']\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "prefix = torch.randn(1, 16, model.config.n_embd, dtype=ptdtype, device=device)\n",
    "\n",
    "max_new_tokens = 5\n",
    "temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        y = model.generate_beam_search(input_ids, max_new_tokens, prefix=prefix, \n",
    "                                       temperature=temperature, beam_width=2)\n",
    "        # print(y)\n",
    "        print(tokenizer.decode(y[0].tolist()))\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2] at entry 0 and [1] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m beam \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m37\u001b[39m, \u001b[38;5;241m38\u001b[39m])\n\u001b[1;32m      2\u001b[0m word_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m192\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbeam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2] at entry 0 and [1] at entry 1"
     ]
    }
   ],
   "source": [
    "beam = torch.tensor([37, 38])\n",
    "word_idx = torch.tensor(192)\n",
    "\n",
    "\n",
    "torch.stack([beam, word_idx.reshape(1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam.shape, word_idx.reshape(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.cat((beam, word_idx.reshape(1)), dim=0)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '<|endoftext|>i love you so much <|endoftext|>'\n",
    "\n",
    "input_ids = gpt2_tokenizer(start,  return_tensors=\"pt\")['input_ids']\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "prefix = torch.randn(1, 32, model.config.n_embd, dtype=ptdtype, device=device)\n",
    "\n",
    "print('Input shapes', input_ids.shape, prefix.shape)\n",
    "\n",
    "loss, logits = model.forward(idx=input_ids, targets=input_ids, prefix=prefix, )\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train().to(device)\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "decode = lambda l: enc.decode(l)\n",
    "\n",
    "\n",
    "start = \"Russian is the best\"\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "prefix = torch.randn(1, 16, model.config.n_embd, dtype=ptdtype, device=device)\n",
    "\n",
    "max_new_tokens = 15\n",
    "temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 20\n",
    "\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(3):\n",
    "            y = model.generate(x, max_new_tokens, prefix=prefix, temperature=temperature, top_k=top_k)\n",
    "            # print(y)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss calculation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's add context vectors into model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forward get into account idxs and also context vectors I did smt similar actually. \n",
    "\n",
    "N-fixed number of brain_tokens. \n",
    "\n",
    "- add into beggining of the sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut our model: remove layers.\n",
    "\n",
    "This is approach to reduce number of layers. which allows to tune models with fewer GPU clusters. Like distillation.\n",
    "\n",
    "\n",
    "So we can distill model for our task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_blocks = list(model.transformer.h.children())\n",
    "cut_blocks = all_blocks[:4] + all_blocks[4:8]\n",
    "model.transformer.h = nn.Sequential(*cut_blocks)\n",
    "count_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 15\n",
    "temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(3):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            # print(y)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [20, 30, 50]\n",
    "l2 = [50, 60]\n",
    "\n",
    "l1 + l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(amodel.transformer.h[:10].children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '<|endoftext|>i love you so much'\n",
    "print(encode(start))\n",
    "\n",
    "input_ids = gpt2_tokenizer(start,  return_tensors=\"pt\")['input_ids']\n",
    "input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'brainformer'\n",
    "\n",
    "train_config = TrainConfig(exp_name='brainformer_simple', \n",
    "                           mixed_precision=False, \n",
    "                           batch_size=16)\n",
    "\n",
    "data_path = Path(r\"D:\\Work\\brain-to-text-competition\\data\\competitionData\")\n",
    "\n",
    "# train_dataset = BrainDataset(data_path / 'train')\n",
    "\n",
    "test_dataset = BrainDataset(data_path / 'test')\n",
    "\n",
    "# submit_dataset = BrainDataset(data_path / 'competitionHoldOut')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(lines):\n",
    "    unique_words = set()\n",
    "    for line in lines:\n",
    "        unique_words.update(line.lower().replace('.', '').split())\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_unique_words(train_dataset.targets)\n",
    "test_set = get_unique_words(test_dataset.targets)\n",
    "\n",
    "intersection = train_set.intersection(test_set)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "print(len(intersection))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import gpt2_model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
