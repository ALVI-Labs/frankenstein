{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from simple_parsing.helpers import Serializable\n",
    "import time\n",
    "import numpy as np\n",
    "## Functions\n",
    "\n",
    "def build_complex_rope_cache(dim: int, seq_len: int, theta: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute cache for RoPE and store on device with complex dtype. \n",
    "    It speeds up computation.\n",
    "    Return: [T, dim//2]\n",
    "    \"\"\"\n",
    "    \n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    t = torch.arange(seq_len)  \n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    cache = torch.polar(torch.ones_like(freqs), freqs) \n",
    "    cache.requires_grad = False\n",
    "    return cache\n",
    "\n",
    "def apply_rope(x: torch.Tensor, rope: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Now, we do not cut rope cache.  You have to cut outside.\n",
    "    x - [b, t, n_h, dim] \n",
    "    rope(freqs_cis):  [T, dim//2] or [B, T, dim//2]\n",
    "    \"\"\"\n",
    "    T = x.size(1)\n",
    "    len_rope = len(rope.shape)\n",
    "    \n",
    "    if len_rope == 2:\n",
    "        rope = rope[:T]\n",
    "    else:\n",
    "        rope = rope[:, :T]\n",
    "\n",
    "    rope = rope.unsqueeze(-2) # [B, T, 1, dim//2] or [T, 1, dim//2]\n",
    "\n",
    "    # b, t, n_h, dim - > (b, t, n_h, dim/2, 2)\n",
    "    x_ = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))    \n",
    "\n",
    "    # (b, t, n_h, dim/2, 2) * t, 1, dim/2\n",
    "    x_out = torch.view_as_real(x_ * rope).flatten(3)\n",
    "    return x_out.type_as(x)\n",
    "\n",
    "def build_advanced_causal_mask(block_size, tok_per_time):\n",
    "    \"\"\"\n",
    "    Return mask in bool dtype, where\n",
    "        True - include in attention \n",
    "        False - don't include.   \n",
    "    \"\"\"\n",
    "\n",
    "    mask = torch.ones(block_size, block_size)\n",
    "    mask = torch.tril(mask)\n",
    "\n",
    "    S = torch.ones(tok_per_time, tok_per_time)\n",
    "\n",
    "    for i in range(0, block_size, tok_per_time):\n",
    "        lp, rp = i, i + tok_per_time\n",
    "        mask[lp:rp, lp:rp] = S\n",
    "    \n",
    "    causal_mask = mask\n",
    "    causal_mask = causal_mask.to(torch.bool)\n",
    "    return causal_mask\n",
    "\n",
    "## Blocks.\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w1 = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(config.hidden_dim, config.dim, bias=False)\n",
    "        self.w3 = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "class CausalSelfAttention(nn.Module): \n",
    "    \"\"\"\n",
    "    Simple Multi head attention with einops and F.scaled_dot_product_attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, is_causal=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert config.n_heads == config.n_kv_heads, \"n_heads should be equal n_kv_heads\"\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "        self.n_kv_heads = config.n_heads # for simplicity we use vanilla transformer.\n",
    "        self.repeats = self.n_heads // self.n_kv_heads\n",
    "        self.head_dim = config.head_dim\n",
    "\n",
    "        self.qw = nn.Linear(config.dim, config.head_dim * config.n_heads, bias=False)\n",
    "        self.kw = nn.Linear(config.dim, config.head_dim * config.n_kv_heads, bias=False)\n",
    "        self.vw = nn.Linear(config.dim, config.head_dim * config.n_kv_heads, bias=False)\n",
    "\n",
    "        self.project = nn.Linear(config.head_dim * config.n_heads, config.dim, bias=False)\n",
    "\n",
    "    def forward(self, x, attn_mask, rope, kv_cache=None):\n",
    "        B, T, C = x.size() # b, t, c*h        \n",
    "        q, k, v = self.qw(x), self.kw(x), self.vw(x) \n",
    "\n",
    "        # split by n_heads.\n",
    "        q = rearrange(q, 'b t (nh c) -> b t nh c', b=B, t=T, nh=self.n_heads, c=self.head_dim)\n",
    "        k = rearrange(k, 'b t (nh c) -> b t nh c', b=B, t=T, nh=self.n_heads, c=self.head_dim)\n",
    "        v = rearrange(v, 'b t (nh c) -> b t nh c', b=B, t=T, nh=self.n_heads, c=self.head_dim)\n",
    "\n",
    "        if rope is not None:\n",
    "            q = apply_rope(q, rope)\n",
    "            k = apply_rope(k, rope)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            t_q, t_k = q.size(1), k.size(1)\n",
    "            attn_mask = attn_mask[..., -t_q:, -t_k:]\n",
    "\n",
    "        q = q.transpose(1, 2)  # (B, nh, T, c)\n",
    "        k = k.transpose(1, 2)  # (B, nh, T, c)\n",
    "        v = v.transpose(1, 2)  # (B, nh, T, c)\n",
    "        \n",
    "        res = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)  \n",
    "\n",
    "        res = rearrange(res, 'b nh t c -> b t (nh c)', b=B, t=T, nh=self.n_heads, c=self.head_dim)\n",
    "        res = self.project(res)\n",
    "\n",
    "        return res\n",
    "    \n",
    "class CausalCrossAttention(nn.Module): \n",
    "    \"\"\"\n",
    "    Simple Multi head attention with einops and F.scaled_dot_product_attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, is_causal=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert config.n_heads == config.n_kv_heads, \"n_heads should be equal n_kv_heads\"\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "        self.n_kv_heads = config.n_heads # for simplicity we use vanilla transformer.\n",
    "        self.repeats = self.n_heads // self.n_kv_heads\n",
    "\n",
    "        self.qw = nn.Linear(config.dim, config.head_dim * config.n_heads, bias=False)\n",
    "        self.kw = nn.Linear(config.dim, config.head_dim * config.n_kv_heads, bias=False)\n",
    "        self.vw = nn.Linear(config.dim, config.head_dim * config.n_kv_heads, bias=False)\n",
    "\n",
    "        self.project = nn.Linear(config.head_dim * config.n_heads, config.dim, bias=False)\n",
    "        # self.block_size = config.block_size\n",
    "\n",
    "        self.kv_cache = None\n",
    "\n",
    "    def forward(self, x, context, attn_mask=None, use_kv_cache=None):\n",
    "        \"\"\"\n",
    "        context should have the same dim as x vectors. \n",
    "        context seqlen >> x seqlen\n",
    "        \"\"\"\n",
    "        B, T, C = x.size() # b, t, c*h \n",
    "        q, k, v = self.qw(x), self.kw(context), self.vw(context) \n",
    "\n",
    "        # split by n_heads.\n",
    "        q = rearrange(q, 'b t (nh c) -> b nh t c', nh = self.n_heads)\n",
    "        k = rearrange(k, 'b t (nh c) -> b nh t c', nh = self.n_heads)\n",
    "        v = rearrange(v, 'b t (nh c) -> b nh t c', nh = self.n_heads)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            t_q, t_k = q.size(2), k.size(2)\n",
    "            attn_mask = attn_mask[..., -t_q:, -t_k:]\n",
    "            \n",
    "        res = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)  \n",
    "\n",
    "        res = rearrange(res, 'b h t c -> b t (h c)')\n",
    "        res = self.project(res)\n",
    "        return res    \n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = RMSNorm(config.dim)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = RMSNorm(config.dim)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, rope=None, kv_cache=False):\n",
    "        x = x + self.attn(self.ln_1(x), attn_mask, rope, kv_cache=kv_cache) \n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "class CrossBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.sa_block = Block(config)\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(config.dim)\n",
    "        self.cross_attn = CausalCrossAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.dim)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, context, self_attn_mask=None, cross_attn_mask=None, sa_rope=None):\n",
    "        \"\"\"\n",
    "        Block with Cross attention -> Block with Self Attention. \n",
    "        \"\"\"\n",
    "        # cross attention\n",
    "        x = x + self.cross_attn(self.ln_1(x), context, attn_mask=cross_attn_mask) \n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        # self attention\n",
    "        x = self.sa_block(x, attn_mask=self_attn_mask, rope=sa_rope)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def create_attention_mask_from_padding(x, pad_value=0):\n",
    "    \"\"\"\n",
    "    Update the attention mask based on padded positions in the input tensor.ns.\n",
    "    \"\"\"\n",
    "    is_padded = (x == pad_value).all(dim=2)  \n",
    "    attn_mask = ~is_padded.unsqueeze(1) & ~is_padded.unsqueeze(2)  # Create the square attention mask\n",
    "\n",
    "    return attn_mask\n",
    "## Models.\n",
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            emb = nn.Linear(config.patch_size, config.dim),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layers)]),\n",
    "            ln_f = nn.LayerNorm(config.dim),\n",
    "        ))    \n",
    "\n",
    "        self.precompute_rope_cash = build_complex_rope_cache(dim=self.config.head_dim,\n",
    "                                                             seq_len=config.block_size,\n",
    "                                                             theta=config.rope_theta)\n",
    "        \n",
    "        self.attn_mask = mask = torch.ones(config.block_size, config.block_size).to(torch.bool)\n",
    "\n",
    "\n",
    "        print(\"Encoder: number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "        print('Shape of the rope cache: ', self.precompute_rope_cash.shape)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, rope_cache=None):\n",
    "        \"\"\"\n",
    "        myo signals with shape: with shape [B, T, C]\n",
    "        \"\"\"\n",
    "        attn_mask = self.attn_mask if attn_mask is None else attn_mask\n",
    "        rope_cache = self.rope_cache if rope_cache is None else rope_cache\n",
    "\n",
    "        # embedding\n",
    "        x = self.transformer.emb(x)\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x, \n",
    "                      attn_mask=attn_mask, \n",
    "                      rope=rope_cache)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return next(self.parameters()).dtype\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def rope_cache(self) -> torch.Tensor:\n",
    "        # Just to use proper device.\n",
    "        if self.precompute_rope_cash.device != self.device:\n",
    "            self.precompute_rope_cash = self.precompute_rope_cash.to(device=self.device)\n",
    "        return self.precompute_rope_cash\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "\n",
    "\n",
    "class SimpleMAE(nn.Module):\n",
    "    def __init__(self, encoder_config, mae_config):\n",
    "        super().__init__()\n",
    "        self.encoder_config = encoder_config\n",
    "        \n",
    "        self.encoder = SimpleEncoder(encoder_config)\n",
    "\n",
    "        self.dim = mae_config.dim\n",
    "\n",
    "        self.decoder = nn.ModuleDict(dict(\n",
    "            emb = nn.Linear(encoder_config.dim, mae_config.dim), # connection between them.\n",
    "            h = nn.ModuleList([Block(mae_config) for _ in range(mae_config.n_layers)]),\n",
    "        ))\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.randn(mae_config.dim))\n",
    "        self.decoder_pos_emb = nn.Embedding(encoder_config.block_size, mae_config.dim)\n",
    "        self.to_signals = nn.Linear(mae_config.dim, encoder_config.patch_size)\n",
    "\n",
    "        print(\"MAE: number of parameters: %.2fM\" % (self.get_num_params()/1e6))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "    def get_masking_indices(self, masking_ratio, x):\n",
    "        b, n_tokens, _ = x.shape\n",
    "\n",
    "        num_masked = int(masking_ratio * n_tokens)\n",
    "        rand_indices = torch.rand(b, n_tokens, device = x.device).argsort(dim=-1) # get idxs of random values.\n",
    "        masked_indices, unmasked_indices = rand_indices[:, :num_masked], rand_indices[:, num_masked:]\n",
    "\n",
    "        masked_indices, _ = torch.sort(masked_indices, dim=1)\n",
    "        unmasked_indices, _ = torch.sort(unmasked_indices, dim=1)      \n",
    "        \n",
    "        return masked_indices, unmasked_indices\n",
    "    \n",
    "\n",
    "    def forward(self, x, targets=None, date_info=None, masking_ratio=0.75, return_preds=False):\n",
    "        \"\"\"\n",
    "        Inputs: x with shape -> B, T, C\n",
    "        \"\"\"\n",
    "        b, t, c = x.shape\n",
    "\n",
    "        ### Preparing attn, rope, masking\n",
    "        masked_indices, unmasked_indices = self.get_masking_indices(masking_ratio, x) # [b, N]\n",
    "        \n",
    "        batch_range = torch.arange(b, device=x.device)[:, None]\n",
    "\n",
    "        is_padded = (x == 0).all(dim=2)\n",
    "        attn_mask = ~is_padded.unsqueeze(1) & ~is_padded.unsqueeze(2)\n",
    "        attn_mask_unmasked = attn_mask[batch_range[..., None], unmasked_indices[..., None], unmasked_indices[:, None, :]]\n",
    "        attn_mask_unmasked = rearrange(attn_mask_unmasked, 'b h w -> b 1 h w')\n",
    "        \n",
    "        \n",
    "        rope_cache = self.encoder.rope_cache.expand(b, -1, -1)\n",
    "        rope_cache_unmasked = rope_cache[batch_range, unmasked_indices]\n",
    "        \n",
    "\n",
    "        ### ENCODER\n",
    "        \n",
    "        tokens = x[batch_range, unmasked_indices]\n",
    "\n",
    "        tokens = self.encoder(tokens, attn_mask=attn_mask_unmasked, rope_cache=rope_cache_unmasked)\n",
    "        \n",
    "\n",
    "        ### DECODER \n",
    "        attn_mask = rearrange(attn_mask, 'b h w -> b 1 h w')\n",
    "\n",
    "        unmasked_decoder_tokens = self.decoder.emb(tokens)\n",
    "\n",
    "        decoder_tokens = torch.zeros(b, t, self.dim, device=x.device, dtype=x.dtype)\n",
    "        decoder_tokens[batch_range, unmasked_indices] = unmasked_decoder_tokens\n",
    "        decoder_tokens[batch_range, masked_indices] = self.mask_token\n",
    "\n",
    "        decoder_pos_emb = self.decoder_pos_emb(torch.cat([unmasked_indices, masked_indices], 1))\n",
    "        decoder_tokens = decoder_tokens + decoder_pos_emb\n",
    "\n",
    "\n",
    "        print('kokens, attn_mask_unmasked, rope_cache_unmasked', decoder_tokens.shape, attn_mask.shape, rope_cache.shape)\n",
    "        for block in self.decoder.h:\n",
    "            decoder_tokens = block(decoder_tokens, attn_mask)\n",
    "\n",
    "        pred_tokens = self.to_signals(decoder_tokens)\n",
    "\n",
    "        \n",
    "        ### LOSS: mse on masked and not padded tokens.\n",
    "    \n",
    "        tokens_pred_masked = pred_tokens[batch_range, masked_indices]\n",
    "        tokens_real_masked = x[batch_range, masked_indices]\n",
    "\n",
    "        # let's calculate loss on masked and not padded tokens.\n",
    "        mask_valid = ~is_padded[batch_range, masked_indices]\n",
    "        loss_tensor = F.mse_loss(tokens_pred_masked, tokens_real_masked, reduction='none')\n",
    "        loss_real_values = loss_tensor[mask_valid.nonzero(as_tuple=True)]\n",
    "        recon_loss = torch.mean(loss_real_values)\n",
    "        if return_preds:\n",
    "            # what we masked?\n",
    "            binary_mask = torch.zeros_like(x, device=x.device, dtype=x.dtype) \n",
    "            binary_mask[batch_range, masked_indices] = 1\n",
    "\n",
    "            reconstruction_signal = torch.zeros_like(x, device=x.device, dtype=x.dtype)\n",
    "            reconstruction_signal[batch_range, masked_indices] = tokens_pred_masked\n",
    "            reconstruction_signal[batch_range, unmasked_indices] = x[batch_range, unmasked_indices]\n",
    "\n",
    "            return recon_loss, reconstruction_signal, binary_mask\n",
    "\n",
    "        return recon_loss, None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SimpleEncoderConfig(Serializable):\n",
    "    # data params\n",
    "    block_size: int = 768\n",
    "    patch_size: int = 128\n",
    "\n",
    "    n_layers: int = 6\n",
    "    dim: int = 256\n",
    "    hidden_dim: int = 1024\n",
    "\n",
    "    head_dim: int = 32\n",
    "    n_heads: int = 4\n",
    "    n_kv_heads: int = 4 # now it should be the same with n_heads.\n",
    "    rope_theta: int = 10000\n",
    "\n",
    "@dataclass\n",
    "class SimpleMAEConfig(Serializable):\n",
    "    # data params\n",
    "    n_layers: int = 2\n",
    "    dim: int = 256\n",
    "    hidden_dim: int = 1024\n",
    "\n",
    "    head_dim: int = 32\n",
    "    n_heads: int = 8\n",
    "    n_kv_heads: int = 8 # now it should be the same with n_heads.\n",
    "    rope_theta: int = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: number of parameters: 5.54M\n",
      "Shape of the rope cache:  torch.Size([768, 16])\n",
      "Encoder: number of parameters: 5.54M\n",
      "Shape of the rope cache:  torch.Size([768, 16])\n",
      "MAE: number of parameters: 7.94M\n"
     ]
    }
   ],
   "source": [
    "config_encoder = SimpleEncoderConfig()\n",
    "config_mae = SimpleMAEConfig()\n",
    "\n",
    "\n",
    "encoder = SimpleEncoder(config_encoder)\n",
    "model = SimpleMAE(config_encoder, config_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kokens, attn_mask_unmasked, rope_cache_unmasked torch.Size([3, 768, 256]) torch.Size([3, 1, 768, 768]) torch.Size([3, 768, 16])\n",
      "tensor(1.6508, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, 768, 128)\n",
    "\n",
    "res = encoder(x)\n",
    "\n",
    "loss, hat, binary = model(x, masking_ratio=0.5, return_preds=True)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21496077fa0>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.ones(16, 500, 256)\n",
    "x[0, -100:] = 0\n",
    "aa\n",
    "attn_mask = torch.ones(500, 500).to(torch.bool)\n",
    "attn_mask = attn_mask.expand(16, -1, -1)\n",
    "\n",
    "attn_mask = create_attention_mask_from_padding(x)\n",
    "\n",
    "plt.imshow(attn_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 500, 500])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# batch_size, seq_length, feature_dim = 16, 500, 256\n",
    "# x = torch.ones(batch_size, seq_length, feature_dim)\n",
    "# x[0, -100:] = 0  # Pad the last 100 tokens of the first sequence\n",
    "\n",
    "# # Get the updated attention mask\n",
    "# attn_mask = update_attention_mask(x)\n",
    "\n",
    "# print(attn_mask.shape)  # Output the shape of the updated mask to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(16, 500, 256)\n",
    "x[0, -100:] = 0  # Pad the last 100 tokens of the first sequence\n",
    "\n",
    "is_padded = (x == 0).all(dim=2)\n",
    "\n",
    "batch_range = torch.arange(16, device=x.device)[:, None]\n",
    "num_masked = 100\n",
    "rand_indices = torch.rand(16, 500, device = x.device).argsort(dim=-1) # get idxs of random values.\n",
    "masked_indices, unmasked_indices = rand_indices[:, :num_masked], rand_indices[:, num_masked:]\n",
    "\n",
    "mask_valid = ~is_padded[batch_range, masked_indices]\n",
    "valid_masked_indices = masked_indices[mask_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pred_masked = x[batch_range, masked_indices]\n",
    "tokens_real_masked = x[batch_range, masked_indices]\n",
    "\n",
    "# #let's calculate loss on masked and not padded tokens.\n",
    "mask_valid = ~is_padded[batch_range, masked_indices]\n",
    "loss_tensor = F.mse_loss(tokens_pred_masked, tokens_real_masked, reduction='none')\n",
    "loss_real_values = loss_tensor[mask_valid.nonzero(as_tuple=True)]\n",
    "recon_loss = torch.mean(loss_real_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
