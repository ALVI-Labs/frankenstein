{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoModelForCausalLM, GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "# BERT Tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# GPT-2 Tokenizer and Model\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2Model.from_pretrained('gpt2', add_cross_attention=True)\n",
    "\n",
    "# Set special tokens\n",
    "bos = gpt2_tokenizer.bos_token\n",
    "eos = gpt2_tokenizer.eos_token\n",
    "gpt2_tokenizer.pad_token = '-100'\n",
    "\n",
    "# Generate random encoder hidden states\n",
    "encoder_hidden_states = torch.rand(1, 16, gpt2_model.config.n_embd)\n",
    "\n",
    "# Tokenize input text\n",
    "txt = bos + 'He want you ask ' + eos\n",
    "input_ids = gpt2_tokenizer(txt, padding=\"max_length\", max_length=16, add_special_tokens=True, return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "# Generate tokens using GPT-2 model\n",
    "gen_tokens = gpt2_model.generate(\n",
    "    input_ids,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "# Load GPT-2 model and tokenizer for generating text\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", add_cross_attention=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Generate text using GPT-2 model\n",
    "prompt = bos\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    encoder_hidden_states=encoder_hidden_states*0,\n",
    "    do_sample=False,\n",
    "    temperature=0.9,\n",
    "    max_length=10,\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "\n",
    "def tokenize(processor, target):\n",
    "    return processor.tokenizer(target, return_tensors=\"pt\").input_ids[0]\n",
    "\n",
    "def untokenize(processor, tokens):\n",
    "    labels = tokens\n",
    "    labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "    return label_str"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
