{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoModelForCausalLM, GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "# BERT Tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# GPT-2 Tokenizer and Model\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2Model.from_pretrained('gpt2', add_cross_attention=True)\n",
    "\n",
    "# Set special tokens\n",
    "bos = gpt2_tokenizer.bos_token\n",
    "eos = gpt2_tokenizer.eos_token\n",
    "gpt2_tokenizer.pad_token = '-100'\n",
    "\n",
    "# Generate random encoder hidden states\n",
    "encoder_hidden_states = torch.rand(1, 16, gpt2_model.config.n_embd)\n",
    "\n",
    "# Tokenize input text\n",
    "txt = bos + 'He want you ask ' + eos\n",
    "input_ids = gpt2_tokenizer(txt, padding=\"max_length\", max_length=16, add_special_tokens=True, return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "# Generate tokens using GPT-2 model\n",
    "gen_tokens = gpt2_model.generate(\n",
    "    input_ids,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "# Load GPT-2 model and tokenizer for generating text\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", add_cross_attention=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Generate text using GPT-2 model\n",
    "prompt = bos\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    encoder_hidden_states=encoder_hidden_states*0,\n",
    "    do_sample=False,\n",
    "    temperature=0.9,\n",
    "    max_length=10,\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "\n",
    "def tokenize(processor, target):\n",
    "    return processor.tokenizer(target, return_tensors=\"pt\").input_ids[0]\n",
    "\n",
    "def untokenize(processor, tokens):\n",
    "    labels = tokens\n",
    "    labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "    return label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        beam_scorer: BeamScorer,\n",
    "        logits_processor: Optional[LogitsProcessorList] = None,\n",
    "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int]]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_scores: Optional[bool] = None,\n",
    "        return_dict_in_generate: Optional[bool] = None,\n",
    "        synced_gpus: bool = False,\n",
    "        **model_kwargs,\n",
    "    ) -> Union[BeamSearchOutput, torch.LongTensor]:\n",
    "        r\"\"\"\n",
    "        Generates sequences of token ids for models with a language modeling head using **beam search decoding** and\n",
    "        can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
    "\n",
    "        <Tip warning={true}>\n",
    "\n",
    "        In most cases, you do not need to call [`~generation.GenerationMixin.beam_search`] directly. Use generate()\n",
    "        instead. For an overview of generation strategies and code examples, check the [following\n",
    "        guide](../generation_strategies).\n",
    "\n",
    "        </Tip>\n",
    "\n",
    "        Parameters:\n",
    "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                The sequence used as a prompt for the generation.\n",
    "            beam_scorer (`BeamScorer`):\n",
    "                An derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n",
    "                sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.\n",
    "            logits_processor (`LogitsProcessorList`, *optional*):\n",
    "                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
    "                used to modify the prediction scores of the language modeling head applied at each generation step.\n",
    "            stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
    "                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
    "                used to tell if the generation loop should stop.\n",
    "            max_length (`int`, *optional*, defaults to 20):\n",
    "                **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
    "                tokens. The maximum length of the sequence to be generated.\n",
    "            pad_token_id (`int`, *optional*):\n",
    "                The id of the *padding* token.\n",
    "            eos_token_id (`Union[int, List[int]]`, *optional*):\n",
    "                The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
    "            output_attentions (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more details.\n",
    "            output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more details.\n",
    "            output_scores (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
    "            return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "            synced_gpus (`bool`, *optional*, defaults to `False`):\n",
    "                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
    "            model_kwargs:\n",
    "                Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n",
    "                an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
    "\n",
    "        Return:\n",
    "            [`generation.BeamSearchDecoderOnlyOutput`], [`~generation.BeamSearchEncoderDecoderOutput`] or\n",
    "            `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "            [`~generation.BeamSearchDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
    "            `return_dict_in_generate=True` or a [`~generation.BeamSearchEncoderDecoderOutput`] if\n",
    "            `model.config.is_encoder_decoder=True`.\n",
    "\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import (\n",
    "        ...     AutoTokenizer,\n",
    "        ...     AutoModelForSeq2SeqLM,\n",
    "        ...     LogitsProcessorList,\n",
    "        ...     MinLengthLogitsProcessor,\n",
    "        ...     BeamSearchScorer,\n",
    "        ... )\n",
    "        >>> import torch\n",
    "\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "        >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "\n",
    "        >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
    "        >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "        >>> # lets run beam search using 3 beams\n",
    "        >>> num_beams = 3\n",
    "        >>> # define decoder start token ids\n",
    "        >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
    "        >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
    "\n",
    "        >>> # add encoder_outputs to model keyword arguments\n",
    "        >>> model_kwargs = {\n",
    "        ...     \"encoder_outputs\": model.get_encoder()(\n",
    "        ...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
    "        ...     )\n",
    "        ... }\n",
    "\n",
    "        >>> # instantiate beam scorer\n",
    "        >>> beam_scorer = BeamSearchScorer(\n",
    "        ...     batch_size=1,\n",
    "        ...     num_beams=num_beams,\n",
    "        ...     device=model.device,\n",
    "        ... )\n",
    "\n",
    "        >>> # instantiate logits processors\n",
    "        >>> logits_processor = LogitsProcessorList(\n",
    "        ...     [\n",
    "        ...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
    "        ...     ]\n",
    "        ... )\n",
    "\n",
    "        >>> outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n",
    "\n",
    "        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        ['Wie alt bist du?']\n",
    "        ```\"\"\"\n",
    "        # init values\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "        if max_length is not None:\n",
    "            warnings.warn(\n",
    "                \"`max_length` is deprecated in this function, use\"\n",
    "                \" `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)\n",
    "        if len(stopping_criteria) == 0:\n",
    "            warnings.warn(\"You don't have defined any stopping_criteria, this will likely loop forever\", UserWarning)\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id\n",
    "        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        output_scores = output_scores if output_scores is not None else self.generation_config.output_scores\n",
    "        output_attentions = (\n",
    "            output_attentions if output_attentions is not None else self.generation_config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.generation_config.output_hidden_states\n",
    "        )\n",
    "        return_dict_in_generate = (\n",
    "            return_dict_in_generate\n",
    "            if return_dict_in_generate is not None\n",
    "            else self.generation_config.return_dict_in_generate\n",
    "        )\n",
    "\n",
    "        batch_size = len(beam_scorer._beam_hyps)\n",
    "        num_beams = beam_scorer.num_beams\n",
    "\n",
    "        batch_beam_size, cur_len = input_ids.shape\n",
    "\n",
    "        if num_beams * batch_size != batch_beam_size:\n",
    "            raise ValueError(\n",
    "                f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
    "            )\n",
    "\n",
    "        # init attention / hidden states / scores tuples\n",
    "        scores = () if (return_dict_in_generate and output_scores) else None\n",
    "        beam_indices = (\n",
    "            tuple(() for _ in range(batch_beam_size)) if (return_dict_in_generate and output_scores) else None\n",
    "        )\n",
    "        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n",
    "        if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "            encoder_hidden_states = (\n",
    "                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "            )\n",
    "\n",
    "        # initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens\n",
    "        # of the first beam are considered to avoid sampling the exact same tokens across all beams.\n",
    "        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
    "        beam_scores[:, 1:] = -1e9\n",
    "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "        this_peer_finished = False  # used by synced_gpus only\n",
    "        while True:\n",
    "            if synced_gpus:\n",
    "                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n",
    "                # The following logic allows an early break if all peers finished generating their sequence\n",
    "                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n",
    "                # send 0.0 if we finished, 1.0 otherwise\n",
    "                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
    "                # did all peers finish? the reduced sum will be 0.0 then\n",
    "                if this_peer_finished_flag.item() == 0.0:\n",
    "                    break\n",
    "\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "            )\n",
    "\n",
    "            if synced_gpus and this_peer_finished:\n",
    "                cur_len = cur_len + 1\n",
    "                continue  # don't waste resources running the code we don't need\n",
    "\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token_scores = nn.functional.log_softmax(\n",
    "                next_token_logits, dim=-1\n",
    "            )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "            next_token_scores_processed = logits_processor(input_ids, next_token_scores)\n",
    "            next_token_scores = next_token_scores_processed + beam_scores[:, None].expand_as(\n",
    "                next_token_scores_processed\n",
    "            )\n",
    "\n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    scores += (next_token_scores_processed,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                    )\n",
    "                    if self.config.is_encoder_decoder:\n",
    "                        cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if self.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            # reshape for beam search\n",
    "            vocab_size = next_token_scores.shape[-1]\n",
    "            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
    "\n",
    "            # Sample 1 + len(eos_token_id) next tokens for each beam so we have at least 1 non eos token per beam.\n",
    "            n_eos_tokens = len(eos_token_id) if eos_token_id else 0\n",
    "            next_token_scores, next_tokens = torch.topk(\n",
    "                next_token_scores, max(2, 1 + n_eos_tokens) * num_beams, dim=1, largest=True, sorted=True\n",
    "            )\n",
    "\n",
    "            next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n",
    "            next_tokens = next_tokens % vocab_size\n",
    "\n",
    "            # stateless\n",
    "            beam_outputs = beam_scorer.process(\n",
    "                input_ids,\n",
    "                next_token_scores,\n",
    "                next_tokens,\n",
    "                next_indices,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                beam_indices=beam_indices,\n",
    "            )\n",
    "\n",
    "            beam_scores = beam_outputs[\"next_beam_scores\"]\n",
    "            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "            beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "            )\n",
    "            if model_kwargs[\"past_key_values\"] is not None:\n",
    "                model_kwargs[\"past_key_values\"] = self._reorder_cache(model_kwargs[\"past_key_values\"], beam_idx)\n",
    "\n",
    "            if return_dict_in_generate and output_scores:\n",
    "                beam_indices = tuple((beam_indices[beam_idx[i]] + (beam_idx[i],) for i in range(len(beam_indices))))\n",
    "\n",
    "            # increase cur_len\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            if beam_scorer.is_done or stopping_criteria(input_ids, scores):\n",
    "                if not synced_gpus:\n",
    "                    break\n",
    "                else:\n",
    "                    this_peer_finished = True\n",
    "\n",
    "        sequence_outputs = beam_scorer.finalize(\n",
    "            input_ids,\n",
    "            beam_scores,\n",
    "            next_tokens,\n",
    "            next_indices,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            max_length=stopping_criteria.max_length,\n",
    "            beam_indices=beam_indices,\n",
    "        )\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if not output_scores:\n",
    "                sequence_outputs[\"sequence_scores\"] = None\n",
    "\n",
    "            if self.config.is_encoder_decoder:\n",
    "                return BeamSearchEncoderDecoderOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    beam_indices=sequence_outputs[\"beam_indices\"],\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    cross_attentions=cross_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                return BeamSearchDecoderOnlyOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    beam_indices=sequence_outputs[\"beam_indices\"],\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                )\n",
    "        else:\n",
    "            return sequence_outputs[\"sequences\"]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
