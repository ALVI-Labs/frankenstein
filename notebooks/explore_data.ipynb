{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some information\n",
    "- Maximum lenght of the brain signals : 919, 906, 594\n",
    "- Maximum number of chars: 66."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "from pathlib import Path\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_signal(brain_list, block_list):\n",
    "    return brain_list\n",
    "\n",
    "def process_text(arr):\n",
    "    return [str.strip() for str in arr]\n",
    "\n",
    "def process_file(data_file):\n",
    "    \n",
    "    data = scipy.io.loadmat(data_file)\n",
    "    date = data_file.stem\n",
    "\n",
    "    n_trials = data['blockIdx'].shape[0]\n",
    "\n",
    "    date_list = [date for _ in range(n_trials)]\n",
    "\n",
    "    brain_list = data['spikePow'][0][:]\n",
    "    block_list =  data['blockIdx'][:]\n",
    "\n",
    "    brain_list = process_signal(brain_list, block_list)\n",
    "    \n",
    "    sentence_list = data['sentenceText']\n",
    "    sentence_list = process_text(sentence_list)\n",
    "\n",
    "    return brain_list, sentence_list, date_list\n",
    "\n",
    "\n",
    "def process_all_files(path):\n",
    "    \n",
    "    data_res = {'brain_list':[], 'sentence_list':[], 'date_list':[]}\n",
    "    \n",
    "    for data_file in sorted(path.glob('*.mat')):\n",
    "\n",
    "        brains, sentences, dates = process_file(data_file)\n",
    "        \n",
    "        data_res['brain_list'].extend(brains)\n",
    "        data_res['sentence_list'].extend(sentences)\n",
    "        data_res['date_list'].extend(dates)\n",
    "\n",
    "    return data_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BrainDataset(Dataset):\n",
    "    def __init__(self, path): \n",
    "\n",
    "        data = process_all_files(path)\n",
    "            \n",
    "        self.max_tokens = 64\n",
    "        self.inputs = data['brain_list']\n",
    "        self.targets = data['sentence_list']\n",
    "        self.date = data['date_list']\n",
    "        \n",
    "        all_labels = []\n",
    "\n",
    "        ## IMPORTANT TO PAD INDEXES WITH -100 values. \n",
    "        # for text in self.targets: \n",
    "            # labels = self.processor.tokenizer(text, padding=\"max_length\", max_length=self.max_tokens).input_ids\n",
    "            # labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "            # all_labels.append(labels)\n",
    "            \n",
    "        self.targets_tokens = self.targets\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \n",
    "        input = self.inputs[idx]\n",
    "        target = self.targets_tokens\n",
    "        date = self.date[idx]\n",
    "                \n",
    "        return input, target, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Work\\brain-to-text-competition\\data\\competitionData\\competitionHoldOut\\t12.2022.05.24.mat\n",
      "D:\\Work\\brain-to-text-competition\\data\\competitionData\\competitionHoldOut\\t12.2022.05.26.mat\n",
      "D:\\Work\\brain-to-text-competition\\data\\competitionData\\competitionHoldOut\\t12.2022.06.02.mat\n",
      "D:\\Work\\brain-to-text-competition\\data\\competitionData\\competitionHoldOut\\t12.2022.06.07.mat\n",
      "D:\\Work\\brain-to-text-competition\\data\\competitionData\\competitionHoldOut\\t12.2022.06.14.mat\n",
      "D:\\Work\\brain-to-text-competition\\data\\competitionData\\competitionHoldOut\\t12.2022.06.16.mat\n",
      "D:\\Work\\brain-to-text-competition\\data\\competitionData\\competitionHoldOut\\t12.2022.06.21.mat\n",
      "D:\\Work\\brain-to-text-competition\\data\\competitionData\\competitionHoldOut\\t12.2022.06.28.mat\n",
      "D:\\Work\\brain-to-text-competition\\data\\competitionData\\competitionHoldOut\\t12.2022.07.05.mat\n",
      "D:\\Work\\brain-to-text-competition\\data\\competitionData\\competitionHoldOut\\t12.2022.07.14.mat\n",
      "D:\\Work\\brain-to-text-competition\\data\\competitionData\\competitionHoldOut\\t12.2022.07.21.mat\n",
      "D:\\Work\\brain-to-text-competition\\data\\competitionData\\competitionHoldOut\\t12.2022.07.27.mat\n",
      "D:\\Work\\brain-to-text-competition\\data\\competitionData\\competitionHoldOut\\t12.2022.08.02.mat\n",
      "D:\\Work\\brain-to-text-competition\\data\\competitionData\\competitionHoldOut\\t12.2022.08.11.mat\n",
      "D:\\Work\\brain-to-text-competition\\data\\competitionData\\competitionHoldOut\\t12.2022.08.13.mat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(r\"D:\\Work\\brain-to-text-competition\\data\\competitionData\")\n",
    "\n",
    "submit_dataset = BrainDataset(data_path / 'competitionHoldOut')\n",
    "len(submit_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare submission file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def process_string(text):\n",
    "    text = text.lower()\n",
    "    punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "    text = ''.join(char for char in text if char not in punctuation)\n",
    "    return text\n",
    "\n",
    "def make_prediction_on_dataset(submit_dataset):\n",
    "    pred_list =[]\n",
    "    for sample in submit_dataset:\n",
    "        pred_text = \"I LOVE you. don't So much AXAX\"\n",
    "        pred_text = process_string(pred_text)\n",
    "        pred_list.append(pred_text)\n",
    "    return pred_list\n",
    "\n",
    "def create_string_file(filename, string_list):\n",
    "    with open(filename, 'w') as file:\n",
    "        for string in string_list:\n",
    "            # Process each string\n",
    "            processed_string = process_string(string)\n",
    "            \n",
    "            # Write the processed string to the file\n",
    "            file.write(processed_string + '\\n')\n",
    "\n",
    "result = make_prediction_file(submit_dataset)\n",
    "create_string_file(\"sub.txt\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel, AutoModelForCausalLM\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Download model and configuration from huggingface.co and cache.\n",
    "# model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "decoder = AutoModelForCausalLM.from_pretrained('google-bert/bert-base-uncased', is_decoder=True, add_cross_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.8.crossattention.c_proj.weight', 'h.7.crossattention.c_proj.bias', 'h.11.crossattention.c_attn.bias', 'h.10.ln_cross_attn.bias', 'h.1.crossattention.c_attn.bias', 'h.10.crossattention.c_proj.weight', 'h.6.ln_cross_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.8.crossattention.c_attn.weight', 'h.10.ln_cross_attn.weight', 'h.11.ln_cross_attn.weight', 'h.5.crossattention.q_attn.bias', 'h.4.ln_cross_attn.bias', 'h.6.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.bias', 'h.4.crossattention.q_attn.weight', 'h.1.ln_cross_attn.weight', 'h.8.ln_cross_attn.weight', 'h.1.crossattention.c_proj.weight', 'h.3.crossattention.c_attn.bias', 'h.10.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.bias', 'h.7.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.5.ln_cross_attn.weight', 'h.11.ln_cross_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.q_attn.bias', 'h.5.crossattention.c_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.q_attn.bias', 'h.0.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.3.crossattention.q_attn.bias', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.4.crossattention.c_proj.weight', 'h.8.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.weight', 'h.8.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.0.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.bias', 'h.3.crossattention.c_proj.bias', 'h.1.crossattention.q_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.0.ln_cross_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.3.crossattention.c_attn.weight', 'h.8.crossattention.q_attn.bias', 'h.2.crossattention.c_attn.bias', 'h.6.ln_cross_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.9.ln_cross_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.q_attn.bias', 'h.6.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.bias', 'h.4.crossattention.c_attn.bias', 'h.2.ln_cross_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.9.ln_cross_attn.bias', 'h.7.crossattention.c_attn.bias', 'h.9.crossattention.c_proj.weight', 'h.1.ln_cross_attn.bias', 'h.5.ln_cross_attn.bias', 'h.0.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.weight', 'h.6.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.4.crossattention.c_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.3.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.3.ln_cross_attn.bias', 'h.2.crossattention.q_attn.bias', 'h.4.ln_cross_attn.weight', 'h.0.ln_cross_attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '-100'}, clean_up_tokenization_spaces=True)\n",
      "tensor([[50256,  1544,   765,   345,  1265,   220, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "bos = tokenizer.bos_token\n",
    "eos = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = GPT2Model.from_pretrained('gpt2', add_cross_attention=True)\n",
    "\n",
    "\n",
    "\n",
    "encoder_hidden_states = torch.rand(1, 16, model.config.n_embd)\n",
    "\n",
    "tokenizer.pad_token = '-100'\n",
    "print(tokenizer)\n",
    "txt = bos + 'He want you ask ' + eos\n",
    "\n",
    "input_ids = tokenizer(txt, padding=\"max_length\", max_length=16, add_special_tokens=True, return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "\n",
    "print(input_ids)\n",
    "result = model(input_ids, encoder_hidden_states=encoder_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (GPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'GPT2LMHeadModel'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[141], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gen_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aleksandr.kovalev\\anaconda3\\envs\\torch_2\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aleksandr.kovalev\\anaconda3\\envs\\torch_2\\lib\\site-packages\\transformers\\generation\\utils.py:1408\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1405\u001b[0m         synced_gpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1407\u001b[0m \u001b[38;5;66;03m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n\u001b[1;32m-> 1408\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;66;03m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001b[39;00m\n\u001b[0;32m   1411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1412\u001b[0m     \u001b[38;5;66;03m# legacy: users may modify the model configuration to control generation -- update the generation config\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m     \u001b[38;5;66;03m# model attribute accordingly, if it was created from the model config\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aleksandr.kovalev\\anaconda3\\envs\\torch_2\\lib\\site-packages\\transformers\\generation\\utils.py:1203\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generate_compatible_classes:\n\u001b[0;32m   1202\u001b[0m     exception_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please use one of the following classes instead: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerate_compatible_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1203\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(exception_message)\n",
      "\u001b[1;31mTypeError\u001b[0m: The current model class (GPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'GPT2LMHeadModel'}"
     ]
    }
   ],
   "source": [
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    encoder_hidden_states=encoder_hidden_states,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7549, 0.8107, 0.4531,  ..., 0.9738, 0.6373, 0.2971],\n",
       "         [0.6407, 0.1286, 0.6991,  ..., 0.4754, 0.1013, 0.5525],\n",
       "         [0.6175, 0.4941, 0.1572,  ..., 0.5079, 0.5169, 0.7434],\n",
       "         ...,\n",
       "         [0.2553, 0.0449, 0.3503,  ..., 0.1883, 0.1110, 0.4542],\n",
       "         [0.0213, 0.1227, 0.3695,  ..., 0.8032, 0.7929, 0.5463],\n",
       "         [0.1097, 0.4807, 0.9345,  ..., 0.4903, 0.9435, 0.9352]]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.8.crossattention.c_proj.weight', 'h.7.crossattention.c_proj.bias', 'h.11.crossattention.c_attn.bias', 'h.10.ln_cross_attn.bias', 'h.1.crossattention.c_attn.bias', 'h.10.crossattention.c_proj.weight', 'h.6.ln_cross_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.8.crossattention.c_attn.weight', 'h.10.ln_cross_attn.weight', 'h.11.ln_cross_attn.weight', 'h.5.crossattention.q_attn.bias', 'h.4.ln_cross_attn.bias', 'h.6.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.bias', 'h.4.crossattention.q_attn.weight', 'h.1.ln_cross_attn.weight', 'h.8.ln_cross_attn.weight', 'h.1.crossattention.c_proj.weight', 'h.3.crossattention.c_attn.bias', 'h.10.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.bias', 'h.7.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.5.ln_cross_attn.weight', 'h.11.ln_cross_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.q_attn.bias', 'h.5.crossattention.c_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.q_attn.bias', 'h.0.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.3.crossattention.q_attn.bias', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.4.crossattention.c_proj.weight', 'h.8.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.weight', 'h.8.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.0.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.bias', 'h.3.crossattention.c_proj.bias', 'h.1.crossattention.q_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.0.ln_cross_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.3.crossattention.c_attn.weight', 'h.8.crossattention.q_attn.bias', 'h.2.crossattention.c_attn.bias', 'h.6.ln_cross_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.9.ln_cross_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.q_attn.bias', 'h.6.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.bias', 'h.4.crossattention.c_attn.bias', 'h.2.ln_cross_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.9.ln_cross_attn.bias', 'h.7.crossattention.c_attn.bias', 'h.9.crossattention.c_proj.weight', 'h.1.ln_cross_attn.bias', 'h.5.ln_cross_attn.bias', 'h.0.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.weight', 'h.6.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.4.crossattention.c_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.3.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.3.ln_cross_attn.bias', 'h.2.crossattention.q_attn.bias', 'h.4.ln_cross_attn.weight', 'h.0.ln_cross_attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>\\nThe first time I saw the new version'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\",  add_cross_attention=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "bos = tokenizer.bos_token\n",
    "\n",
    "prompt = bos\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "print(input_ids)\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    encoder_hidden_states=encoder_hidden_states*0,\n",
    "    do_sample=False,\n",
    "    temperature=0.9,\n",
    "    max_length=10,\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "\n",
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>: The best way to get a real-life tattoo artist to do the job: Get your tattoo done in a small office in the suburbs of New York.\\n\\nThe best way to get a real-life tattoo artist to do the job: Get your tattoo done in a small office in the suburbs of New York. Photo: Courtesy of Sueden Tattooing\\n\\nKurt H. Williams, owner of Sueden Tattooing, is the owner of Sueden'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(processor, target):\n",
    "    return processor.tokenizer(target, return_tensors=\"pt\").input_ids[0]\n",
    "\n",
    "def untokenize(processor, tokens):\n",
    "    labels = tokens\n",
    "    labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "    return label_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
