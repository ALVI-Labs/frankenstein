{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from simple_parsing.helpers import Serializable\n",
    "import time\n",
    "import numpy as np\n",
    "## Functions\n",
    "\n",
    "def build_complex_rope_cache(dim: int, seq_len: int, theta: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute cache for RoPE and store on device with complex dtype. \n",
    "    It speeds up computation.\n",
    "    Return: [T, dim//2]\n",
    "    \"\"\"\n",
    "    \n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    t = torch.arange(seq_len)  \n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    cache = torch.polar(torch.ones_like(freqs), freqs) \n",
    "    cache.requires_grad = False\n",
    "    return cache\n",
    "\n",
    "def apply_rope(x: torch.Tensor, rope: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Now, we do not cut rope cache.  You have to cut outside.\n",
    "    x - [b, t, n_h, dim] \n",
    "    rope(freqs_cis):  [T, dim//2] or [B, T, dim//2]\n",
    "    \"\"\"\n",
    "    T = x.size(1)\n",
    "    len_rope = len(rope.shape)\n",
    "    \n",
    "    if len_rope == 2:\n",
    "        rope = rope[:T]\n",
    "    else:\n",
    "        rope = rope[:, :T]\n",
    "\n",
    "    rope = rope.unsqueeze(-2) # [B, T, 1, dim//2] or [T, 1, dim//2]\n",
    "\n",
    "    # b, t, n_h, dim - > (b, t, n_h, dim/2, 2)\n",
    "    x_ = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))    \n",
    "\n",
    "    # (b, t, n_h, dim/2, 2) * t, 1, dim/2\n",
    "    x_out = torch.view_as_real(x_ * rope).flatten(3)\n",
    "    return x_out.type_as(x)\n",
    "\n",
    "def build_advanced_causal_mask(block_size, tok_per_time):\n",
    "    \"\"\"\n",
    "    Return mask in bool dtype, where\n",
    "        True - include in attention \n",
    "        False - don't include.   \n",
    "    \"\"\"\n",
    "\n",
    "    mask = torch.ones(block_size, block_size)\n",
    "    mask = torch.tril(mask)\n",
    "\n",
    "    S = torch.ones(tok_per_time, tok_per_time)\n",
    "\n",
    "    for i in range(0, block_size, tok_per_time):\n",
    "        lp, rp = i, i + tok_per_time\n",
    "        mask[lp:rp, lp:rp] = S\n",
    "    \n",
    "    causal_mask = mask\n",
    "    causal_mask = causal_mask.to(torch.bool)\n",
    "    return causal_mask\n",
    "\n",
    "## Blocks.\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w1 = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(config.hidden_dim, config.dim, bias=False)\n",
    "        self.w3 = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "class CausalSelfAttention(nn.Module): \n",
    "    \"\"\"\n",
    "    Simple Multi head attention with einops and F.scaled_dot_product_attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, is_causal=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert config.n_heads == config.n_kv_heads, \"n_heads should be equal n_kv_heads\"\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "        self.n_kv_heads = config.n_heads # for simplicity we use vanilla transformer.\n",
    "        self.repeats = self.n_heads // self.n_kv_heads\n",
    "        self.head_dim = config.head_dim\n",
    "\n",
    "        self.qw = nn.Linear(config.dim, config.head_dim * config.n_heads, bias=False)\n",
    "        self.kw = nn.Linear(config.dim, config.head_dim * config.n_kv_heads, bias=False)\n",
    "        self.vw = nn.Linear(config.dim, config.head_dim * config.n_kv_heads, bias=False)\n",
    "\n",
    "        self.project = nn.Linear(config.head_dim * config.n_heads, config.dim, bias=False)\n",
    "\n",
    "    def forward(self, x, attn_mask, rope, kv_cache=None):\n",
    "        B, T, C = x.size() # b, t, c*h        \n",
    "        q, k, v = self.qw(x), self.kw(x), self.vw(x) \n",
    "\n",
    "        # split by n_heads.\n",
    "        q = rearrange(q, 'b t (nh c) -> b t nh c', b=B, t=T, nh=self.n_heads, c=self.head_dim)\n",
    "        k = rearrange(k, 'b t (nh c) -> b t nh c', b=B, t=T, nh=self.n_heads, c=self.head_dim)\n",
    "        v = rearrange(v, 'b t (nh c) -> b t nh c', b=B, t=T, nh=self.n_heads, c=self.head_dim)\n",
    "\n",
    "        if rope is not None:\n",
    "            q = apply_rope(q, rope)\n",
    "            k = apply_rope(k, rope)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            t_q, t_k = q.size(1), k.size(1)\n",
    "            attn_mask = attn_mask[..., -t_q:, -t_k:]\n",
    "\n",
    "        q = q.transpose(1, 2)  # (B, nh, T, c)\n",
    "        k = k.transpose(1, 2)  # (B, nh, T, c)\n",
    "        v = v.transpose(1, 2)  # (B, nh, T, c)\n",
    "        \n",
    "        res = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)  \n",
    "\n",
    "        res = rearrange(res, 'b nh t c -> b t (nh c)', b=B, t=T, nh=self.n_heads, c=self.head_dim)\n",
    "        res = self.project(res)\n",
    "\n",
    "        return res\n",
    "    \n",
    "class CausalCrossAttention(nn.Module): \n",
    "    \"\"\"\n",
    "    Simple Multi head attention with einops and F.scaled_dot_product_attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, is_causal=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert config.n_heads == config.n_kv_heads, \"n_heads should be equal n_kv_heads\"\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "        self.n_kv_heads = config.n_heads # for simplicity we use vanilla transformer.\n",
    "        self.repeats = self.n_heads // self.n_kv_heads\n",
    "\n",
    "        self.qw = nn.Linear(config.dim, config.head_dim * config.n_heads, bias=False)\n",
    "        self.kw = nn.Linear(config.dim, config.head_dim * config.n_kv_heads, bias=False)\n",
    "        self.vw = nn.Linear(config.dim, config.head_dim * config.n_kv_heads, bias=False)\n",
    "\n",
    "        self.project = nn.Linear(config.head_dim * config.n_heads, config.dim, bias=False)\n",
    "        # self.block_size = config.block_size\n",
    "\n",
    "        self.kv_cache = None\n",
    "\n",
    "    def forward(self, x, context, attn_mask=None, use_kv_cache=None):\n",
    "        \"\"\"\n",
    "        context should have the same dim as x vectors. \n",
    "        context seqlen >> x seqlen\n",
    "        \"\"\"\n",
    "        B, T, C = x.size() # b, t, c*h \n",
    "        q, k, v = self.qw(x), self.kw(context), self.vw(context) \n",
    "\n",
    "        # split by n_heads.\n",
    "        q = rearrange(q, 'b t (nh c) -> b nh t c', nh = self.n_heads)\n",
    "        k = rearrange(k, 'b t (nh c) -> b nh t c', nh = self.n_heads)\n",
    "        v = rearrange(v, 'b t (nh c) -> b nh t c', nh = self.n_heads)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            t_q, t_k = q.size(2), k.size(2)\n",
    "            attn_mask = attn_mask[..., -t_q:, -t_k:]\n",
    "            \n",
    "        res = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)  \n",
    "\n",
    "        res = rearrange(res, 'b h t c -> b t (h c)')\n",
    "        res = self.project(res)\n",
    "        return res    \n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = RMSNorm(config.dim)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = RMSNorm(config.dim)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, rope=None, kv_cache=False):\n",
    "        x = x + self.attn(self.ln_1(x), attn_mask, rope, kv_cache=kv_cache) \n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "class CrossBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.sa_block = Block(config)\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(config.dim)\n",
    "        self.cross_attn = CausalCrossAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.dim)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, context, self_attn_mask=None, cross_attn_mask=None, sa_rope=None):\n",
    "        \"\"\"\n",
    "        Block with Cross attention -> Block with Self Attention. \n",
    "        \"\"\"\n",
    "        # cross attention\n",
    "        x = x + self.cross_attn(self.ln_1(x), context, attn_mask=cross_attn_mask) \n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        # self attention\n",
    "        x = self.sa_block(x, attn_mask=self_attn_mask, rope=sa_rope)\n",
    "\n",
    "        return x\n",
    "\n",
    "## Models.\n",
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            emb = nn.Linear(config.patch_size, config.dim),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layers)]),\n",
    "            ln_f = nn.LayerNorm(config.dim),\n",
    "        ))\n",
    "        n_registers = 4\n",
    "        self.registers = nn.Parameter(torch.zeros(1, n_registers, config.dim))\n",
    "\n",
    "        self.precompute_rope_cash = build_complex_rope_cache(dim=self.config.head_dim,\n",
    "                                                             seq_len=config.block_size+n_registers,\n",
    "                                                             theta=config.rope_theta)\n",
    "        \n",
    "        self.attn_mask = torch.ones(config.block_size, config.block_size).to(torch.bool)\n",
    "        \n",
    "        print(\"Encoder: number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "        print('Shape of the rope cache: ', self.precompute_rope_cash.shape)\n",
    "\n",
    "    def forward(self, x, attn_mask=None, rope_cache=None):\n",
    "        \"\"\"\n",
    "        myo signals with shape: with shape [B, T, C]\n",
    "        \"\"\"\n",
    "        attn_mask = self.attn_mask if attn_mask is None else attn_mask\n",
    "        rope_cache = self.rope_cache if rope_cache is None else torch.cat([rope_cache, self.rope_cache[-4:]])\n",
    "\n",
    "        # embedding\n",
    "        x = self.transformer.emb(x)\n",
    "\n",
    "        # print(attn_mask.shape)\n",
    "        p2d = (4,0, 0, 4)\n",
    "        if len(attn_mask.shape) == 2:\n",
    "            attn_mask = F.pad(attn_mask[None, :].to(torch.int), p2d, \"replicate\").to(torch.bool)[0]\n",
    "        else:\n",
    "            attn_mask = F.pad(attn_mask.to(torch.int), p2d, \"replicate\").to(torch.bool)\n",
    "        \n",
    "        x = torch.cat([self.registers.expand(x.size(0), -1, -1), x], dim=1)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x, \n",
    "                      attn_mask=attn_mask, \n",
    "                      rope=rope_cache)\n",
    "        x = x[:, [0], :]\n",
    "        x = self.transformer.ln_f(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        return next(self.parameters()).dtype\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def rope_cache(self) -> torch.Tensor:\n",
    "        # Just to use proper device.\n",
    "        if self.precompute_rope_cash.device != self.device:\n",
    "            self.precompute_rope_cash = self.precompute_rope_cash.to(device=self.device)\n",
    "        return self.precompute_rope_cash\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "\n",
    "\n",
    "class SimpleMAE(nn.Module):\n",
    "    def __init__(self, encoder_config, mae_config):\n",
    "        super().__init__()\n",
    "        self.encoder_config = encoder_config\n",
    "        \n",
    "        self.encoder = SimpleEncoder(encoder_config)\n",
    "\n",
    "        self.dim = mae_config.dim\n",
    "\n",
    "        self.decoder = nn.ModuleDict(dict(\n",
    "            emb = nn.Linear(encoder_config.dim, mae_config.dim), # connection between them.\n",
    "            h = nn.ModuleList([Block(mae_config) for _ in range(mae_config.n_layers)]),\n",
    "        ))\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.randn(mae_config.dim))\n",
    "        self.decoder_pos_emb = nn.Embedding(encoder_config.block_size, mae_config.dim)\n",
    "        self.to_signals = nn.Linear(mae_config.dim, encoder_config.patch_size)\n",
    "\n",
    "        print(\"MAE: number of parameters: %.2fM\" % (self.get_num_params()/1e6))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "    def get_masking_indices(self, masking_ratio, x):\n",
    "        b, n_tokens, _ = x.shape\n",
    "\n",
    "        num_masked = int(masking_ratio * n_tokens)\n",
    "        rand_indices = torch.rand(b, n_tokens, device = x.device).argsort(dim=-1) # get idxs of random values.\n",
    "        masked_indices, unmasked_indices = rand_indices[:, :num_masked], rand_indices[:, num_masked:]\n",
    "\n",
    "        masked_indices, _ = torch.sort(masked_indices, dim=1)\n",
    "        unmasked_indices, _ = torch.sort(unmasked_indices, dim=1)      \n",
    "        \n",
    "        return masked_indices, unmasked_indices\n",
    "    \n",
    "\n",
    "    def forward(self, x, targets=None, date_info=None, masking_ratio=0.75, return_preds=False):\n",
    "        \"\"\"\n",
    "        Inputs: x with shape -> B, T, C\n",
    "        \"\"\"\n",
    "        b, t, c = x.shape\n",
    "\n",
    "        ### Preparing attn, rope, masking\n",
    "        masked_indices, unmasked_indices = self.get_masking_indices(masking_ratio, x) # [b, N]\n",
    "        \n",
    "        batch_range = torch.arange(b, device=x.device)[:, None]\n",
    "\n",
    "        is_padded = (x == 0).all(dim=2)\n",
    "    \n",
    "        attn_mask = ~is_padded.unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        # attn_mask = ~is_padded.unsqueeze(1) & ~is_padded.unsqueeze(2)\n",
    "        \n",
    "        attn_mask_unmasked = attn_mask[batch_range[..., None], unmasked_indices[..., None], unmasked_indices[:, None, :]]\n",
    "        attn_mask_unmasked = rearrange(attn_mask_unmasked, 'b h w -> b 1 h w')\n",
    "        \n",
    "        rope_cache = self.encoder.rope_cache.expand(b, -1, -1)\n",
    "        rope_cache_unmasked = rope_cache[batch_range, unmasked_indices]\n",
    "        \n",
    "\n",
    "        ### ENCODER\n",
    "        \n",
    "        tokens = x[batch_range, unmasked_indices]\n",
    "        print('attn_mask_unmasked', attn_mask_unmasked.shape, attn_mask_unmasked[0])\n",
    "        plt.imshow(attn_mask_unmasked[0, 0].detach().cpu())\n",
    "        plt.colorbar()\n",
    "        \n",
    "        tokens = self.encoder(tokens, attn_mask=attn_mask_unmasked)\n",
    "        print('TOKENS AFTER ENCODER', tokens[0])\n",
    "\n",
    "        ### DECODER \n",
    "        attn_mask = rearrange(attn_mask, 'b h w -> b 1 h w')\n",
    "\n",
    "        unmasked_decoder_tokens = self.decoder.emb(tokens)\n",
    "\n",
    "        decoder_tokens = torch.zeros(b, t, self.dim, device=x.device, dtype=x.dtype)\n",
    "        decoder_tokens[batch_range, unmasked_indices] = unmasked_decoder_tokens\n",
    "        decoder_tokens[batch_range, masked_indices] = self.mask_token\n",
    "\n",
    "        decoder_pos_emb = self.decoder_pos_emb(torch.cat([unmasked_indices, masked_indices], 1))\n",
    "        decoder_tokens = decoder_tokens + decoder_pos_emb\n",
    "\n",
    "\n",
    "        \n",
    "        for block in self.decoder.h:\n",
    "            decoder_tokens = block(decoder_tokens, attn_mask)\n",
    "\n",
    "\n",
    "        pred_tokens = self.to_signals(decoder_tokens)\n",
    "\n",
    "        ### LOSS: mse on masked and not padded tokens.\n",
    "        tokens_pred_masked = pred_tokens[batch_range, masked_indices]\n",
    "        tokens_real_masked = x[batch_range, masked_indices]\n",
    "\n",
    "        # let's calculate loss on masked and not padded tokens.\n",
    "        mask_valid = ~is_padded[batch_range, masked_indices]\n",
    "        loss_tensor = F.mse_loss(tokens_pred_masked, tokens_real_masked, reduction='none')\n",
    "        loss_real_values = loss_tensor[mask_valid.nonzero(as_tuple=True)]\n",
    "        recon_loss = torch.mean(loss_real_values)\n",
    "    \n",
    "        # print(loss_tensor)\n",
    "        \n",
    "        if return_preds:\n",
    "            # what we masked?\n",
    "            binary_mask = torch.zeros_like(x, device=x.device, dtype=x.dtype) \n",
    "            binary_mask[batch_range, masked_indices] = 1\n",
    "\n",
    "            reconstruction_signal = torch.zeros_like(x, device=x.device, dtype=x.dtype)\n",
    "            reconstruction_signal[batch_range, masked_indices] = tokens_pred_masked\n",
    "            reconstruction_signal[batch_range, unmasked_indices] = x[batch_range, unmasked_indices]\n",
    "\n",
    "            return recon_loss, reconstruction_signal, binary_mask\n",
    "\n",
    "        return recon_loss, None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SimpleEncoderConfig(Serializable):\n",
    "    # data params\n",
    "    block_size: int = 768\n",
    "    patch_size: int = 256\n",
    "\n",
    "    n_layers: int = 8\n",
    "    dim: int = 256\n",
    "    hidden_dim: int = 1024\n",
    "\n",
    "    head_dim: int = 32\n",
    "    n_heads: int = 8\n",
    "    n_kv_heads: int = 8 # now it should be the same with n_heads.\n",
    "    rope_theta: int = 10000\n",
    "\n",
    "@dataclass\n",
    "class SimpleMAEConfig(Serializable):\n",
    "    # data params\n",
    "    n_layers: int = 2\n",
    "    dim: int = 256\n",
    "    hidden_dim: int = 1024\n",
    "\n",
    "    head_dim: int = 32\n",
    "    n_heads: int = 8\n",
    "    n_kv_heads: int = 8 # now it should be the same with n_heads.\n",
    "    rope_theta: int = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: number of parameters: 8.46M\n",
      "Shape of the rope cache:  torch.Size([772, 16])\n",
      "Encoder: number of parameters: 8.46M\n",
      "Shape of the rope cache:  torch.Size([772, 16])\n",
      "MAE: number of parameters: 10.89M\n"
     ]
    }
   ],
   "source": [
    "config_encoder = SimpleEncoderConfig()\n",
    "config_mae = SimpleMAEConfig()\n",
    "\n",
    "\n",
    "encoder = SimpleEncoder(config_encoder)\n",
    "model = SimpleMAE(config_encoder, config_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0614, -0.8527, -0.0131,  0.2511,  1.7222, -0.9707, -0.4454,  1.9348,\n",
      "         -0.5555,  0.4280,  1.0776, -0.7255, -0.1756,  1.8478,  1.1336, -1.4594,\n",
      "         -0.7389, -0.3855,  0.2015, -0.4919, -0.5759, -0.1446, -1.4060, -0.0833,\n",
      "          1.5510, -0.0935,  0.6183, -0.8087, -0.1450, -1.4213,  1.2143, -0.0130,\n",
      "          0.0945,  2.1195,  1.5054,  1.1011, -1.0506,  0.6405, -3.6995, -0.2645,\n",
      "          0.5529, -1.5450,  0.4029,  0.2878, -0.8788, -2.8001, -0.0520, -0.6037,\n",
      "         -0.6412,  0.7179, -0.5020,  0.1735, -1.1191, -0.4324,  1.2902, -1.1570,\n",
      "          0.8923,  0.7668, -1.3746, -1.4607, -1.1328, -0.0674, -0.8309,  0.9193,\n",
      "         -1.2053, -0.3872, -0.0339,  1.4230,  0.5670, -2.1425, -2.3868, -1.7833,\n",
      "          2.1186,  0.9040,  1.7843, -1.9357,  0.0729,  0.3532, -0.9870,  0.5536,\n",
      "          0.0984,  1.0222,  1.2415,  1.6489,  0.3906,  0.5046, -1.5518,  0.0055,\n",
      "          0.5146,  0.9057, -0.0863, -0.9297,  0.2377,  1.2931, -0.0516, -0.4413,\n",
      "         -0.0800,  0.2380,  0.4937, -0.0938,  0.4963,  0.3423,  0.1978,  1.2173,\n",
      "         -1.4681,  0.5735, -0.4060,  0.3839, -0.2334, -0.6380,  0.1080,  0.2032,\n",
      "         -2.2595,  0.8465,  0.9290, -0.2608,  0.1426,  0.9872,  1.6906, -0.8232,\n",
      "         -0.5542,  0.9973,  0.7126,  0.1339,  1.2749,  0.2692, -0.1417,  0.1861,\n",
      "         -1.2663,  1.2150,  1.2356,  0.8588, -0.1293, -0.7656,  2.6964, -0.8398,\n",
      "          0.8346,  0.4479, -0.0113, -1.2424, -0.9473,  0.1560,  1.3246,  0.1157,\n",
      "          0.0135, -1.5721, -1.0380,  0.2470,  0.1228,  1.9943,  0.6936, -0.3635,\n",
      "         -1.3339,  0.9563, -0.5696, -1.5177, -0.3498,  1.3734, -1.2269,  0.1702,\n",
      "          0.9080,  0.2371, -0.4229, -1.1228,  0.6482,  1.3308, -1.1956,  0.9281,\n",
      "          0.3367,  0.8620, -0.0715,  0.0865,  0.5205,  0.2299,  1.4783, -1.5856,\n",
      "          0.9318, -0.2572, -0.2625,  0.5576,  1.4701,  0.7081, -0.3993,  1.8031,\n",
      "         -0.2285, -0.0498,  0.7566,  1.0425,  2.1546, -0.6468, -1.0872, -0.7504,\n",
      "          1.3037,  1.0676, -0.2439, -1.4721, -0.5917,  0.0770,  0.1922, -0.4450,\n",
      "         -0.9317, -0.3992,  0.1521, -0.4153, -1.4470, -0.0310, -0.1690, -0.4096,\n",
      "          0.1889,  0.1906,  0.0228, -0.6275, -0.3542, -1.1393, -0.6575,  1.1034,\n",
      "         -0.8753,  0.5329, -0.2362, -0.7110,  1.2413,  0.5658, -0.5941, -1.1551,\n",
      "         -0.2980, -0.9322, -1.8360, -1.2035, -0.4643,  0.4249, -0.2980, -2.7883,\n",
      "         -0.0045, -1.3607,  1.5148, -0.4974,  0.6807, -1.3928,  0.8559,  1.4517,\n",
      "         -0.4196,  0.5037,  1.2416,  0.8250,  0.5541, -0.9092,  0.6039, -0.7651,\n",
      "          0.3373,  0.2117,  0.7629,  1.1746, -0.9282, -0.7276, -0.6069,  0.7933]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "attn_mask_unmasked torch.Size([3, 1, 192, 192]) tensor([[[ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         ...,\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False]]])\n",
      "TOKENS AFTER ENCODER tensor([[-4.5211e-01, -1.5687e-01,  1.2655e-02,  3.4047e-01,  9.6249e-01,\n",
      "          7.8378e-01, -5.6428e-01,  8.6290e-01,  1.1096e-01,  1.3453e+00,\n",
      "          9.0877e-01, -1.3817e+00,  4.7462e-01,  8.6938e-01,  1.1155e+00,\n",
      "         -2.0753e-01, -1.4942e+00, -1.5248e+00, -2.8134e-01,  2.4342e+00,\n",
      "          6.1282e-01, -1.0502e+00, -1.8338e+00,  3.1921e-01, -1.2930e+00,\n",
      "          8.0613e-01, -1.5368e+00,  2.3062e+00,  1.2678e+00, -8.7251e-01,\n",
      "          1.2808e+00,  8.9272e-01,  1.2291e+00,  1.7222e-02, -4.6733e-01,\n",
      "          9.3323e-01,  8.1998e-01,  4.5977e-01, -1.2167e+00, -1.2605e+00,\n",
      "          3.0658e-01, -3.4787e-01, -5.8244e-01, -6.4305e-01, -2.2081e+00,\n",
      "         -6.4568e-01, -1.8376e+00, -1.7792e-01, -9.4523e-02,  3.4124e-01,\n",
      "         -1.3823e+00, -1.0336e+00, -8.3318e-01,  1.3719e+00, -1.2737e+00,\n",
      "         -7.9632e-01,  1.3770e+00,  1.2153e+00,  4.4500e-01, -1.6413e+00,\n",
      "          4.2892e-01,  1.2497e+00,  1.5822e-01, -4.4472e-01, -1.3244e+00,\n",
      "         -6.1435e-01,  4.2778e-01, -6.5027e-01,  1.4372e+00, -6.3866e-01,\n",
      "          3.6571e-02,  7.1611e-01, -8.3067e-03, -3.4395e-01, -6.8298e-01,\n",
      "         -2.3370e-01,  8.8449e-02, -1.1587e-02,  2.2937e+00,  3.7516e-01,\n",
      "         -9.0131e-02, -5.1092e-01, -5.0662e-01,  8.7599e-01,  2.0641e-01,\n",
      "          7.2196e-01, -3.4414e-01, -6.7433e-01, -5.4796e-01, -1.2302e+00,\n",
      "         -8.0783e-01,  7.7727e-01, -2.2735e+00, -1.1514e-01, -5.1088e-01,\n",
      "          4.8497e-01, -2.7067e+00, -1.0947e-01, -6.9652e-01,  4.2294e-01,\n",
      "         -4.8133e-01, -3.6959e-01,  3.4521e-01, -1.8968e+00, -7.4949e-01,\n",
      "         -7.8326e-01, -7.4961e-01,  1.3033e+00,  2.1408e-01,  5.8771e-01,\n",
      "          1.7031e+00,  8.2420e-01,  2.3527e+00, -1.9329e+00,  1.9995e+00,\n",
      "          9.1065e-01, -4.3728e-02,  1.3786e+00, -6.7188e-01,  6.3742e-01,\n",
      "          1.6442e+00,  1.8600e-01, -9.7967e-01,  2.4038e-01, -1.6512e-01,\n",
      "          6.2580e-01,  7.2761e-01,  2.4232e-01,  8.7375e-03, -1.7575e+00,\n",
      "         -1.3778e+00, -8.9985e-01,  1.5457e+00,  1.2354e-01, -8.6646e-01,\n",
      "          4.0324e-01,  9.0506e-01,  7.2951e-01,  3.7800e-01, -4.9530e-01,\n",
      "          6.2863e-01,  4.3840e-01, -7.9660e-01, -6.0901e-01, -7.8439e-01,\n",
      "         -1.0445e+00,  9.4066e-01, -5.5573e-01,  1.6203e-01, -1.0921e+00,\n",
      "         -2.0005e+00,  1.5158e+00,  1.5871e+00,  1.9658e+00, -4.8578e-01,\n",
      "         -4.5017e-01, -9.3049e-01,  9.8815e-01, -4.9994e-01,  1.8713e+00,\n",
      "         -1.1443e+00, -1.8942e+00, -7.2884e-01,  9.8368e-01, -1.7364e+00,\n",
      "          4.1728e-01, -1.0552e+00,  4.8806e-01, -9.2652e-01,  1.4546e+00,\n",
      "          7.1040e-01, -8.4522e-01, -1.9463e-01, -1.4647e+00, -8.8544e-01,\n",
      "          3.3977e-01,  7.1532e-01,  1.4093e+00, -3.8386e-01, -8.1548e-01,\n",
      "         -7.0441e-03,  5.6599e-01, -8.9042e-01, -4.6415e-01,  2.0712e-01,\n",
      "         -5.3940e-01, -5.4748e-02, -2.5584e-01, -9.5337e-01, -4.3395e-01,\n",
      "          1.2567e+00, -7.0667e-01,  4.3999e-01,  1.6079e-03,  2.2855e-01,\n",
      "         -8.3637e-01, -1.3699e-02,  1.0274e+00,  1.3120e+00, -2.2788e+00,\n",
      "         -7.2421e-01,  3.5115e-01,  9.3036e-01, -7.6822e-01,  1.4538e+00,\n",
      "         -4.0231e-01, -1.1238e+00, -2.1196e-01, -9.6875e-02, -5.3901e-01,\n",
      "         -1.4710e+00, -1.8800e+00, -1.0196e-01,  6.6833e-01, -7.6831e-01,\n",
      "          1.4025e+00, -3.8514e-04,  1.7848e-01, -7.9138e-01, -1.9319e-01,\n",
      "          1.0724e+00,  1.0079e+00,  3.8815e-01,  6.0787e-01,  4.6356e-01,\n",
      "          6.2851e-01, -1.2704e-01,  4.8141e-01,  1.0554e-01, -1.0028e+00,\n",
      "          8.0004e-01,  3.7809e-01,  2.8998e-01,  1.4997e+00,  1.6513e-01,\n",
      "          8.6741e-02, -2.3103e-01,  3.0071e+00,  3.4303e-01,  2.2241e-01,\n",
      "          1.2630e+00,  7.4610e-01, -1.9110e-01, -2.1456e+00, -3.5318e-01,\n",
      "          9.6934e-02, -1.8483e-01, -2.9047e-01,  2.0380e-01,  1.0698e+00,\n",
      "         -1.3574e+00,  6.2533e-01,  1.0910e+00,  8.1566e-01,  2.0273e+00,\n",
      "          3.5840e-02]], grad_fn=<SelectBackward0>)\n",
      "tensor(1.6559, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAGiCAYAAAD+w19eAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzlElEQVR4nO3dfXBUVZ7/8U8Tkg4PSTRE0omGGFkoHcMykIwSGBSYMRoRRBgBsQRGYGRAGAw4GimHaPkzrrMirgjiyuOIA+UMoC6UGEZ5WsZaCODwVBo1kqCJGVjMA0gndJ/fH5he24SQpLvp9O33q+pW0afP7fvtm1DffM85916bMcYIAACEvA7BDgAAAPgHSR0AAIsgqQMAYBEkdQAALIKkDgCARZDUAQCwCJI6AAAWQVIHAMAiSOoAAFgESR0AAIsgqQMA4Gc7d+7UiBEjlJycLJvNpk2bNl1ynx07digjI0PR0dG67rrr9Oqrr7b6uEFN6kuWLFFaWpqio6OVkZGhXbt2BTMcAAD84syZM+rbt68WL17cov4lJSW68847NXjwYB04cEBPPPGEZs+erb/+9a+tOq4tWA90Wb9+vR544AEtWbJEgwYN0rJly/T666/r6NGj6tGjRzBCAgDA72w2mzZu3KhRo0ZdtM9jjz2md955R8eOHfO0TZ8+XR9//LH+/ve/t/xYwUrqN998s/r376+lS5d62m644QaNGjVKBQUFze7rdrv19ddfKyYmRjabLdChAgD8zBijmpoaJScnq0OHwA0anzt3TnV1dT5/jjGmUb6x2+2y2+2X3LclSf2WW25Rv3799NJLL3naNm7cqLFjx+rs2bOKjIxsUZwdW9TLz+rq6lRUVKTHH3/cqz07O1t79uxp1N/pdMrpdHpef/XVV/rJT34S8DgBAIFVVlama665JiCffe7cOaWldlVFpcvnz+ratatqa2u92hYsWKD8/HyfP1uSKioqlJiY6NWWmJio8+fP6+TJk0pKSmrR5wQlqZ88eVIul6vJL1BRUdGof0FBgZ566qlG7cf3X6vYrqz1A9A+/O6rn+mre7vKdfJksENp986rXru1RTExMQE7Rl1dnSoqXSopSlVsTNtzRXWNW2kZx1VWVqbY2FhPe0uq9Nb48UhAw0B6a0akg5LUGzT1BZoKPi8vT7m5uZ7X1dXVSklJUWzXDj79oADAn6K6RqljhyjZbC0bKg1r30/8Xo4p1NgY/+SK2NhYr6TuTw6Ho1FRW1lZqY4dO6pbt24t/pygJPWEhARFREQ0+QV+XL1LLZ+3AADgx1zGLZcPq8dcxu2/YC4iKytL7777rlfb+++/r8zMzBbPp0tBuqQtKipKGRkZKiws9GovLCzUwIEDgxESAMCi3DI+b61VW1urgwcP6uDBg5IuXLJ28OBBlZaWSrowAj1x4kRP/+nTp+v48ePKzc3VsWPHtGLFCi1fvlzz5s1r1XGDNvyem5urBx54QJmZmcrKytJrr72m0tJSTZ8+PVghAQAsyC23fKm127L3vn37NHToUM/rhinkSZMmadWqVSovL/ckeElKS0vTli1b9Mgjj+iVV15RcnKy/uM//kNjxoxp1XGDltTHjRunU6dO6emnn1Z5ebnS09O1ZcsWpaamBiskAAD8YsiQIWruivFVq1Y1arv11lu1f/9+n44b1IVyM2bM0IwZM4IZAgDA4lzGyOXDLVl82fdyC2pSBwAg0No6L/7D/UMF14MBAGARVOoAAEtzy8gVJpU6SR0AYGkMvwMAgJBDpQ4AsDRWvwMAYBHu7zdf9g8VDL8DAGARVOoAAEtz+bj63Zd9LzeSOgDA0lxGPj6lzX+xBBpJHQBgacypAwCAkEOlDgCwNLdscsnm0/6hgqQOALA0t7mw+bJ/qGD4HQAAi6BSBwBYmsvH4Xdf9r3cSOoAAEsLp6TO8DsAABZBpQ4AsDS3scltfFj97sO+lxtJHQBgaQy/AwCAkEOlDgCwNJc6yOVDDevyYyyBRlIHAFia8XFO3TCnDgBA+8CcOgAACDlU6gAAS3OZDnIZH+bUQ+je7yR1AICluWWT24eBabdCJ6sz/A4AgEVQqQMALC2cFsqR1AEAlub7nDrD7wAA4DKjUgcAWNqFhXI+PNCF4XcAANoHt4+3iWX1OwAAuOyo1AEAlhZOC+VI6gAAS3OrQ9jcfIakDgCwNJexyeXDk9Z82fdy8/ucekFBgX72s58pJiZG3bt316hRo/TJJ5949Zk8ebJsNpvXNmDAAH+HAgBAWPF7Ut+xY4dmzpypjz76SIWFhTp//ryys7N15swZr3533HGHysvLPduWLVv8HQoAAHJ9v/rdly1U+H34/b333vN6vXLlSnXv3l1FRUW65ZZbPO12u10Oh8PfhwcAwIvbdJDbh4Vy7hBaKBfwPz+qqqokSfHx8V7t27dvV/fu3dW7d29NmzZNlZWVF/0Mp9Op6upqrw0AAHgLaFI3xig3N1c///nPlZ6e7mnPycnR2rVr9cEHH+iFF17Q3r17NWzYMDmdziY/p6CgQHFxcZ4tJSUlkGEDACyE4Xc/efjhh/WPf/xDu3fv9mofN26c59/p6enKzMxUamqqNm/erNGjRzf6nLy8POXm5npeV1dXk9gBAC3ilm8r2N3+CyXgApbUZ82apXfeeUc7d+7UNddc02zfpKQkpaamqri4uMn37Xa77HZ7IMIEAMAy/J7UjTGaNWuWNm7cqO3btystLe2S+5w6dUplZWVKSkrydzgAgDDn+81nwnj4febMmXrzzTf19ttvKyYmRhUVFZKkuLg4derUSbW1tcrPz9eYMWOUlJSkL7/8Uk888YQSEhJ0zz33+DscAECY8/02sWGc1JcuXSpJGjJkiFf7ypUrNXnyZEVEROjQoUNas2aNvv32WyUlJWno0KFav369YmJi/B0OAABhIyDD783p1KmTtm7d6u/DAgDQJJ6nDgCARTD8DgCARfh6rXkoXaceOpECAIBmUakDACzNbWxy+3LzmRB69CpJHQBgaW4fh99D6Tr10IkUAAA0i0odAGBpvj96NXTqX5I6AMDSXLLJ5cO15r7se7mFzp8fAACgWVTqAABLY/gdAACLcMm3IXSX/0IJuND58wMAADSLSh0AYGkMvwMAYBHh9ECX0IkUAIA2MN8/erWtm2njfPySJUuUlpam6OhoZWRkaNeuXc32X7t2rfr27avOnTsrKSlJv/71r3Xq1KlWHZOkDgCAn61fv15z5szR/PnzdeDAAQ0ePFg5OTkqLS1tsv/u3bs1ceJETZkyRUeOHNFbb72lvXv3aurUqa06LkkdAGBpDcPvvmyttXDhQk2ZMkVTp07VDTfcoEWLFiklJUVLly5tsv9HH32ka6+9VrNnz1ZaWpp+/vOf66GHHtK+fftadVySOgDA0hqe0ubLJknV1dVem9PpbPJ4dXV1KioqUnZ2tld7dna29uzZ0+Q+AwcO1IkTJ7RlyxYZY/TNN9/oL3/5i4YPH96q70pSBwCgBVJSUhQXF+fZCgoKmux38uRJuVwuJSYmerUnJiaqoqKiyX0GDhyotWvXaty4cYqKipLD4dAVV1yhl19+uVUxsvodAGBpLh8fvdqwb1lZmWJjYz3tdru92f1sNu8FdsaYRm0Njh49qtmzZ+sPf/iDbr/9dpWXl+vRRx/V9OnTtXz58hbHSlIHAFjaD4fQ27q/JMXGxnol9YtJSEhQREREo6q8srKyUfXeoKCgQIMGDdKjjz4qSfrXf/1XdenSRYMHD9YzzzyjpKSkFsXK8DsAAH4UFRWljIwMFRYWerUXFhZq4MCBTe5z9uxZdejgnZIjIiIkXajwW4pKHQBgaW51kNuHGrYt++bm5uqBBx5QZmamsrKy9Nprr6m0tFTTp0+XJOXl5emrr77SmjVrJEkjRozQtGnTtHTpUs/w+5w5c3TTTTcpOTm5xcclqQMALM1lbHL5MPzeln3HjRunU6dO6emnn1Z5ebnS09O1ZcsWpaamSpLKy8u9rlmfPHmyampqtHjxYs2dO1dXXHGFhg0bpn/7t39r1XFtpjV1fTtRXV2tuLg4nf70OsXGMIMAoH146ESWyoZ3leuf/wx2KO3eeVOv7XpbVVVVLZqnbouGXPHbXaNl7xrZ5s9x1tZr6eANAY3VX6jUAQCW5q+FcqGApA4AsDTj41PaTAg90IWkDgCwNJdscrXxoSwN+4eK0PnzAwAANItKHQBgaW7j27y4O4SWk5PUAQCW5vZxTt2XfS+30IkUAAA0i0odAGBpbtnk9mGxmy/7Xm4kdQCApQXjjnLBwvA7AAAWQaUOALC0cFooR1IHAFiaWz7eJjaE5tT9/udHfn6+bDab1+ZwODzvG2OUn5+v5ORkderUSUOGDNGRI0f8HQYAAGEnIGMKN954o8rLyz3boUOHPO89//zzWrhwoRYvXqy9e/fK4XDotttuU01NTSBCAQCEOfP96ve2biaEKvWADL937NjRqzpvYIzRokWLNH/+fI0ePVqStHr1aiUmJurNN9/UQw89FIhwAABhLJye0haQSr24uFjJyclKS0vT+PHj9cUXX0iSSkpKVFFRoezsbE9fu92uW2+9VXv27Lno5zmdTlVXV3ttAAC0RMNCOV+2UOH3SG+++WatWbNGW7du1X/+53+qoqJCAwcO1KlTp1RRUSFJSkxM9NonMTHR815TCgoKFBcX59lSUlL8HTYAACHP78PvOTk5nn/36dNHWVlZ6tmzp1avXq0BAwZIkmw276EMY0yjth/Ky8tTbm6u53V1dTWJHQDQIgy/+1GXLl3Up08fFRcXe+bZf1yVV1ZWNqref8hutys2NtZrAwCgJXxZJOfrLWYvt4AndafTqWPHjikpKUlpaWlyOBwqLCz0vF9XV6cdO3Zo4MCBgQ4FAABL8/vw+7x58zRixAj16NFDlZWVeuaZZ1RdXa1JkybJZrNpzpw5evbZZ9WrVy/16tVLzz77rDp37qwJEyb4OxQAAMJq+N3vSf3EiRO67777dPLkSV111VUaMGCAPvroI6WmpkqSfv/73+u7777TjBkzdPr0ad188816//33FRMT4+9QAAAgqfti3bp1zb5vs9mUn5+v/Px8fx8aAICwxr3fAQCWRqUOAIBFhFNSD53b5AAAgGZRqQMALM3It8enGv+FEnAkdQCApYXT8DtJHQBgaeGU1JlTBwDAIqjUAQCWFk6VOkkdAGBp4ZTUGX4HAMAiqNQBAJZmjE3Gh2rbl30vN5I6AMDSfH0mOs9TBwAAlx2VOgDA0sJpoRxJHQBgaeE0p87wOwAAFkGlDgCwNIbfAQCwiHAafiepAwAszfhYqYdSUmdOHQAAi6BSBwBYmpFkjG/7hwqSOgDA0tyyycYd5QAAQCihUgcAWBqr3wEAsAi3sckWJtepM/wOAIBFUKkDACzNGB9Xv4fQ8neSOgDA0sJpTp3hdwAALIJKHQBgaeFUqZPUAQCWFk6r30nqAABLC6eFcsypAwBgEVTqAABLu1Cp+zKn7sdgAoykDgCwtHBaKMfwOwAAFkGlDgCwNCPfnokeQqPv/q/Ur732WtlstkbbzJkzJUmTJ09u9N6AAQP8HQYAAJL+b/jdly1U+L1S37t3r1wul+f14cOHddttt+nee+/1tN1xxx1auXKl53VUVJS/wwAAIOz4PalfddVVXq+fe+459ezZU7feequnzW63y+Fw+PvQAAA0Fkbj7wFdKFdXV6c33nhDDz74oGy2/xu+2L59u7p3767evXtr2rRpqqysbPZznE6nqqurvTYAAFrE16H3Ng6/L1myRGlpaYqOjlZGRoZ27drVbH+n06n58+crNTVVdrtdPXv21IoVK1p1zIAulNu0aZO+/fZbTZ482dOWk5Oje++9V6mpqSopKdGTTz6pYcOGqaioSHa7vcnPKSgo0FNPPRXIUAEAFhWMO8qtX79ec+bM0ZIlSzRo0CAtW7ZMOTk5Onr0qHr06NHkPmPHjtU333yj5cuX61/+5V9UWVmp8+fPt+q4NmMCd1n97bffrqioKL377rsX7VNeXq7U1FStW7dOo0ePbrKP0+mU0+n0vK6urlZKSopOf3qdYmO4Kg9A+/DQiSyVDe8q1z//GexQ2r3zpl7b9baqqqoUGxsbkGNUV1crLi5OaSvnq0Pn6DZ/jvvsOZX8+v+1Ktabb75Z/fv319KlSz1tN9xwg0aNGqWCgoJG/d977z2NHz9eX3zxheLj49sca8Ay4vHjx7Vt2zZNnTq12X5JSUlKTU1VcXHxRfvY7XbFxsZ6bQAAtIS/Vr//eBr4h8XmD9XV1amoqEjZ2dle7dnZ2dqzZ0+T+7zzzjvKzMzU888/r6uvvlq9e/fWvHnz9N1337XquwZs+H3lypXq3r27hg8f3my/U6dOqaysTElJSYEKBQAQznyYF/fsLyklJcWrecGCBcrPz2/U/eTJk3K5XEpMTPRqT0xMVEVFRZOH+OKLL7R7925FR0dr48aNOnnypGbMmKH//d//bdW8ekCSutvt1sqVKzVp0iR17Ph/h6itrVV+fr7GjBmjpKQkffnll3riiSeUkJCge+65JxChAADgF2VlZV4jxRdbB9bghwvEJckY06itgdvtls1m09q1axUXFydJWrhwoX71q1/plVdeUadOnVoUY0CS+rZt21RaWqoHH3zQqz0iIkKHDh3SmjVr9O233yopKUlDhw7V+vXrFRMTE4hQAABhzl8L5Vo6/ZuQkKCIiIhGVXllZWWj6r1BUlKSrr76ak9Cly7MwRtjdOLECfXq1atFsQYkqWdnZ6up9XedOnXS1q1bA3FIAACadpmvU4+KilJGRoYKCwu9RqELCwt19913N7nPoEGD9NZbb6m2tlZdu3aVJH366afq0KGDrrnmmhYfm6XjAAD4WW5url5//XWtWLFCx44d0yOPPKLS0lJNnz5dkpSXl6eJEyd6+k+YMEHdunXTr3/9ax09elQ7d+7Uo48+qgcffLDFQ+8SD3QBAFhcMB69Om7cOJ06dUpPP/20ysvLlZ6eri1btig1NVXShcu5S0tLPf27du2qwsJCzZo1S5mZmerWrZvGjh2rZ555plXHJakDAKwvCLd6nTFjhmbMmNHke6tWrWrUdv3116uwsNCnYzL8DgCARVCpAwAsLRjD78FCUgcAWFsYPaWNpA4AsDjb95sv+4cG5tQBALAIKnUAgLUx/A4AgEWEUVJn+B0AAIugUgcAWJufHr0aCkjqAABL89dT2kIBw+8AAFgElToAwNrCaKEcSR0AYG1hNKfO8DsAABZBpQ4AsDSbubD5sn+oIKkDAKyNOXUAACyCOXUAABBqqNQBANbG8DsAABYRRkmd4XcAACyCSh0AYG1hVKmT1AEA1sbqdwAAEGqo1AEAlsYd5QAAsIowmlNn+B0AAIsgqQMAYBEMvwMALM0mH+fU/RZJ4JHUAQDWxiVtAAAg1FCpAwCsLYxWv5PUAQDWFkZJneF3AAAsotVJfefOnRoxYoSSk5Nls9m0adMmr/eNMcrPz1dycrI6deqkIUOG6MiRI159nE6nZs2apYSEBHXp0kUjR47UiRMnfPoiAAA0peGOcr5soaLVSf3MmTPq27evFi9e3OT7zz//vBYuXKjFixdr7969cjgcuu2221RTU+PpM2fOHG3cuFHr1q3T7t27VVtbq7vuuksul6vt3wQAgKYYP2whotVz6jk5OcrJyWnyPWOMFi1apPnz52v06NGSpNWrVysxMVFvvvmmHnroIVVVVWn58uX605/+pF/+8peSpDfeeEMpKSnatm2bbr/9dh++DgAA4cuvc+olJSWqqKhQdna2p81ut+vWW2/Vnj17JElFRUWqr6/36pOcnKz09HRPnx9zOp2qrq722gAAaJEwqtT9mtQrKiokSYmJiV7tiYmJnvcqKioUFRWlK6+88qJ9fqygoEBxcXGeLSUlxZ9hAwAsjDl1H9ls3nffMcY0avux5vrk5eWpqqrKs5WVlfktVgAArMKvSd3hcEhSo4q7srLSU707HA7V1dXp9OnTF+3zY3a7XbGxsV4bAAAt0nCbWF+2EOHXpJ6WliaHw6HCwkJPW11dnXbs2KGBAwdKkjIyMhQZGenVp7y8XIcPH/b0AQDAb8JoTr3Vq99ra2v12WefeV6XlJTo4MGDio+PV48ePTRnzhw9++yz6tWrl3r16qVnn31WnTt31oQJEyRJcXFxmjJliubOnatu3bopPj5e8+bNU58+fTyr4QEA8Bdf58VDaU691Ul93759Gjp0qOd1bm6uJGnSpElatWqVfv/73+u7777TjBkzdPr0ad188816//33FRMT49nnxRdfVMeOHTV27Fh99913+sUvfqFVq1YpIiLCD18JAIDwZDPGhNDfIBdUV1crLi5Opz+9TrEx3OkWQPvw0IkslQ3vKtc//xnsUNq986Ze2/W2qqqqArZOqiFXXPeHZ9UhOrrNn+M+d05fPP1EQGP1Fx7oAgCwNl8vSwuh0pcyFwAAi6BSBwBYWxg9epWkDgCwtjBK6gy/AwBgEVTqAABLC6fr1KnUAQCwCJI6AAAWwfA7AMDawmihHEkdAGBp4TSnTlIHAFhfCCVmXzCnDgCARVCpAwCsjTl1AACsIZzm1Bl+BwDAIqjUAQDWxvA7AADWwPA7AAAIOSR1AIC1GT9sbbBkyRKlpaUpOjpaGRkZ2rVrV4v2++///m917NhRP/3pT1t9TJI6AMDagpDU169frzlz5mj+/Pk6cOCABg8erJycHJWWlja7X1VVlSZOnKhf/OIXrT+oSOoAALRIdXW11+Z0Oi/ad+HChZoyZYqmTp2qG264QYsWLVJKSoqWLl3a7DEeeughTZgwQVlZWW2KkaQOALC0hoVyvmySlJKSori4OM9WUFDQ5PHq6upUVFSk7Oxsr/bs7Gzt2bPnonGuXLlSn3/+uRYsWNDm78rqdwCAtfnpkraysjLFxsZ6mu12e5PdT548KZfLpcTERK/2xMREVVRUNLlPcXGxHn/8ce3atUsdO7Y9NZPUAQDW5qekHhsb65XUL8Vms3l/jDGN2iTJ5XJpwoQJeuqpp9S7d28fAiWpAwDgVwkJCYqIiGhUlVdWVjaq3iWppqZG+/bt04EDB/Twww9Lktxut4wx6tixo95//30NGzasRccmqQMALO1y33wmKipKGRkZKiws1D333ONpLyws1N13392of2xsrA4dOuTVtmTJEn3wwQf6y1/+orS0tBYfm6QOALC2INwmNjc3Vw888IAyMzOVlZWl1157TaWlpZo+fbokKS8vT1999ZXWrFmjDh06KD093Wv/7t27Kzo6ulH7pZDUAQDws3HjxunUqVN6+umnVV5ervT0dG3ZskWpqamSpPLy8ktes94WNmNMCN3V9oLq6mrFxcXp9KfXKTaGq/IAtA8PnchS2fCucv3zn8EOpd07b+q1XW+rqqqqVYvPWqMhV9zw8LOKsEe3+XNcznM6tviJgMbqL1TqAABrC6OntFHmAgBgEVTqAABrC6NKnaQOALA02/ebL/uHCobfAQCwCCp1AIC1MfwOAIA1XO47ygUTSR0AYG1hVKkzpw4AgEW0Oqnv3LlTI0aMUHJysmw2mzZt2uR5r76+Xo899pj69OmjLl26KDk5WRMnTtTXX3/t9RlDhgyRzWbz2saPH+/zlwEAoEnGhy2EtDqpnzlzRn379tXixYsbvXf27Fnt379fTz75pPbv368NGzbo008/1ciRIxv1nTZtmsrLyz3bsmXL2vYNAABoRsOcui9bqGj1nHpOTo5ycnKafC8uLk6FhYVebS+//LJuuukmlZaWqkePHp72zp07y+FwtOiYTqdTTqfT87q6urq1YQMAYHkBn1OvqqqSzWbTFVdc4dW+du1aJSQk6MYbb9S8efNUU1Nz0c8oKChQXFycZ0tJSQlw1AAAy/Bl6D3EhuADuvr93LlzevzxxzVhwgSvJ9vcf//9SktLk8Ph0OHDh5WXl6ePP/64UZXfIC8vT7m5uZ7X1dXVJHYAQItwSZsf1NfXa/z48XK73VqyZInXe9OmTfP8Oz09Xb169VJmZqb279+v/v37N/osu90uu90eqFABALCEgAy/19fXa+zYsSopKVFhYeElnz/bv39/RUZGqri4OBDhAADCGcPvbdeQ0IuLi/Xhhx+qW7dul9znyJEjqq+vV1JSkr/DAQCEOYbfm1FbW6vPPvvM87qkpEQHDx5UfHy8kpOT9atf/Ur79+/Xf/3Xf8nlcqmiokKSFB8fr6ioKH3++edau3at7rzzTiUkJOjo0aOaO3eu+vXrp0GDBvnvmwEAEGZandT37dunoUOHel43LGCbNGmS8vPz9c4770iSfvrTn3rt9+GHH2rIkCGKiorS3/72N7300kuqra1VSkqKhg8frgULFigiIsKHrwIAQBPC6DaxrU7qQ4YMkTEX/4bNvSdJKSkp2rFjR2sPCwBA25DUAQCwhnCaU+eBLgAAWASVOgDA2hh+BwDAGmzGyHaJ9V6X2j9UMPwOAIBFUKkDAKyN4XcAAKyB1e8AACDkUKkDAKyN4XcAAKyB4XcAABByqNQBANbG8DsAANYQTsPvJHUAgLWFUaXOnDoAABZBpQ4AsLxQGkL3BUkdAGBtxlzYfNk/RDD8DgCARVCpAwAsjdXvAABYBavfAQBAqKFSBwBYms19YfNl/1BBUgcAWBvD7wAAINRQqQMALI3V7wAAWEUY3XyGpA4AsLRwqtSZUwcAwCKo1AEA1hZGq99J6gAAS2P4HQAAhBwqdQCAtbH6HQAAa2D4HQAAhBwqdQCAtbH6HQAAa2D4HQAAhJxWJ/WdO3dqxIgRSk5Ols1m06ZNm7zenzx5smw2m9c2YMAArz5Op1OzZs1SQkKCunTpopEjR+rEiRM+fREAAJrkNr5vIaLVSf3MmTPq27evFi9efNE+d9xxh8rLyz3bli1bvN6fM2eONm7cqHXr1mn37t2qra3VXXfdJZfL1fpvAABAc4wfthDR6jn1nJwc5eTkNNvHbrfL4XA0+V5VVZWWL1+uP/3pT/rlL38pSXrjjTeUkpKibdu26fbbb2+0j9PplNPp9Lyurq5ubdgAgDBlk49z6n6LJPACMqe+fft2de/eXb1799a0adNUWVnpea+oqEj19fXKzs72tCUnJys9PV179uxp8vMKCgoUFxfn2VJSUgIRNgAAIc3vST0nJ0dr167VBx98oBdeeEF79+7VsGHDPJV2RUWFoqKidOWVV3rtl5iYqIqKiiY/My8vT1VVVZ6trKzM32EDAKyq4Y5yvmwhwu+XtI0bN87z7/T0dGVmZio1NVWbN2/W6NGjL7qfMUY2W9ODHHa7XXa73d+hAgDCAJe0+VFSUpJSU1NVXFwsSXI4HKqrq9Pp06e9+lVWVioxMTHQ4QAAcFksWbJEaWlpio6OVkZGhnbt2nXRvhs2bNBtt92mq666SrGxscrKytLWrVtbfcyAJ/VTp06prKxMSUlJkqSMjAxFRkaqsLDQ06e8vFyHDx/WwIEDAx0OACDcBGH1+/r16zVnzhzNnz9fBw4c0ODBg5WTk6PS0tIm++/cuVO33XabtmzZoqKiIg0dOlQjRozQgQMHWnXcVg+/19bW6rPPPvO8Likp0cGDBxUfH6/4+Hjl5+drzJgxSkpK0pdffqknnnhCCQkJuueeeyRJcXFxmjJliubOnatu3bopPj5e8+bNU58+fTyr4QEA8BebMbL5MC/esO+Pr7xqbmp44cKFmjJliqZOnSpJWrRokbZu3aqlS5eqoKCgUf9FixZ5vX722Wf19ttv691331W/fv1aHGurK/V9+/apX79+noPk5uaqX79++sMf/qCIiAgdOnRId999t3r37q1Jkyapd+/e+vvf/66YmBjPZ7z44osaNWqUxo4dq0GDBqlz58569913FRER0dpwAAC4LFJSUryuxGoqOUtSXV2dioqKvK7ykqTs7OyLXuX1Y263WzU1NYqPj29VjK2u1IcMGSLTzF88LZkDiI6O1ssvv6yXX365tYcHAKB13N9vvuwvqaysTLGxsZ7mi1XpJ0+elMvlarROrLmrvH7shRde0JkzZzR27NhWhcoDXQAAluav4ffY2FivpH7J/X50RVdzV3n90J///Gfl5+fr7bffVvfu3VsVK0kdAAA/SkhIUERERKOqvCVXea1fv15TpkzRW2+91aZ1ZjylDQBgbZd59XtUVJQyMjK8rvKSpMLCwmav8vrzn/+syZMn680339Tw4cNbd9DvUakDAKzN17vCtWHf3NxcPfDAA8rMzFRWVpZee+01lZaWavr06ZIu3Cn1q6++0po1ayRdSOgTJ07USy+9pAEDBniq/E6dOikuLq7FxyWpAwAsLRh3lBs3bpxOnTqlp59+WuXl5UpPT9eWLVuUmpoq6cL9WX54zfqyZct0/vx5zZw5UzNnzvS0T5o0SatWrWrxcUnqAAAEwIwZMzRjxowm3/txot6+fbtfjklSBwBYWxCG34OFpA4AsDSb+8Lmy/6hgtXvAABYBJU6AMDaGH4HAMAi2vikNa/9QwTD7wAAWASVOgDA0vx17/dQQFIHAFhbGM2pM/wOAIBFUKkDAKzNyLfnqYdOoU5SBwBYG3PqAABYhZGPc+p+iyTgmFMHAMAiqNQBANYWRqvfSeoAAGtzS7L5uH+IYPgdAACLoFIHAFgaq98BALCKMJpTZ/gdAACLoFIHAFhbGFXqJHUAgLWFUVJn+B0AAIugUgcAWFsYXadOUgcAWBqXtAEAYBXMqQMAgFBDpQ4AsDa3kWw+VNvu0KnUSeoAAGtj+B0AAIQaKnUAgMX5WKkrdCp1kjoAwNoYfr+4nTt3asSIEUpOTpbNZtOmTZu83rfZbE1uf/zjHz19hgwZ0uj98ePH+/xlAAAIZ61O6mfOnFHfvn21ePHiJt8vLy/32lasWCGbzaYxY8Z49Zs2bZpXv2XLlrXtGwAA0By38X0LEa0efs/JyVFOTs5F33c4HF6v3377bQ0dOlTXXXedV3vnzp0b9QUAwO+M+8Lmy/4hIqCr37/55htt3rxZU6ZMafTe2rVrlZCQoBtvvFHz5s1TTU3NRT/H6XSqurraawMAAN4CulBu9erViomJ0ejRo73a77//fqWlpcnhcOjw4cPKy8vTxx9/rMLCwiY/p6CgQE899VQgQwUAWFUYLZQLaFJfsWKF7r//fkVHR3u1T5s2zfPv9PR09erVS5mZmdq/f7/69+/f6HPy8vKUm5vreV1dXa2UlJTABQ4AsA63kU+XpVl5Tr2ldu3apU8++UTr16+/ZN/+/fsrMjJSxcXFTSZ1u90uu90eiDABAFYXRpV6wObUly9froyMDPXt2/eSfY8cOaL6+nolJSUFKhwAACyv1ZV6bW2tPvvsM8/rkpISHTx4UPHx8erRo4ekC8Pjb731ll544YVG+3/++edau3at7rzzTiUkJOjo0aOaO3eu+vXrp0GDBvnwVQAAaIKRj5W63yIJuFYn9X379mno0KGe1w1z3ZMmTdKqVaskSevWrZMxRvfdd1+j/aOiovS3v/1NL730kmpra5WSkqLhw4drwYIFioiIaOPXAADgIsJo+L3VSX3IkCEyl/iCv/nNb/Sb3/ymyfdSUlK0Y8eO1h4WAABcAvd+BwBYm9styYcbyLhD5+YzJHUAgLWF0fA7z1MHAMAiqNQBANYWRpU6SR0AYG1hdEc5ht8BALAIKnUAgKUZ45bx4fGpvux7uZHUAQDWZoxvQ+jMqQMA0E4YH+fUQyipM6cOAIBFUKkDAKzN7ZZsPsyLM6cOAEA7wfA7AAAINVTqAABLM263jA/D71zSBgBAe8HwOwAACDVU6gAAa3MbyRYelTpJHQBgbcZI8uWSttBJ6gy/AwBgEVTqAABLM24j48PwuwmhSp2kDgCwNuOWb8PvoXNJG8PvAABLM27j89YWS5YsUVpamqKjo5WRkaFdu3Y123/Hjh3KyMhQdHS0rrvuOr366qutPiZJHQAAP1u/fr3mzJmj+fPn68CBAxo8eLBycnJUWlraZP+SkhLdeeedGjx4sA4cOKAnnnhCs2fP1l//+tdWHTckh98b5jeqa0NnSASA9dXV1um8u04uUx/sUNq987pwji7HfPV54/RpCL0h1urqaq92u90uu93e5D4LFy7UlClTNHXqVEnSokWLtHXrVi1dulQFBQWN+r/66qvq0aOHFi1aJEm64YYbtG/fPv37v/+7xowZ0/JgTQgqKytruD0QGxsbG1sIb2VlZQHLFd99951xOBx+ibNr166N2hYsWNDkcZ1Op4mIiDAbNmzwap89e7a55ZZbmtxn8ODBZvbs2V5tGzZsMB07djR1dXUt/s4hWaknJyfr6NGj+slPfqKysjLFxsYGO6RWq66uVkpKSsjGL4X+dyD+4CL+4Ap2/MYY1dTUKDk5OWDHiI6OVklJierq6nz+LGOMbDabV9vFqvSTJ0/K5XIpMTHRqz0xMVEVFRVN7lNRUdFk//Pnz+vkyZNKSkpqUZwhmdQ7dOigq6++WpIUGxsbkv+hGoR6/FLofwfiDy7iD65gxh8XFxfwY0RHRys6Ojrgx2nKj/8IaOoPg0v1b6q9OSyUAwDAjxISEhQREdGoKq+srGxUjTdwOBxN9u/YsaO6devW4mOT1AEA8KOoqChlZGSosLDQq72wsFADBw5scp+srKxG/d9//31lZmYqMjKyxccO2aRut9u1YMGCi85ptHehHr8U+t+B+IOL+IMr1ONv73Jzc/X6669rxYoVOnbsmB555BGVlpZq+vTpkqS8vDxNnDjR03/69Ok6fvy4cnNzdezYMa1YsULLly/XvHnzWnVcmzEhdP87AABCxJIlS/T888+rvLxc6enpevHFF3XLLbdIkiZPnqwvv/xS27dv9/TfsWOHHnnkER05ckTJycl67LHHPH8EtBRJHQAAiwjZ4XcAAOCNpA4AgEWQ1AEAsAiSOgAAFhGySb21j7QLloKCAv3sZz9TTEyMunfvrlGjRumTTz7x6jN58mTZbDavbcCAAUGK2Ft+fn6j2BwOh+d9Y4zy8/OVnJysTp06aciQITpy5EgQI/Z27bXXNorfZrNp5syZktrfud+5c6dGjBih5ORk2Ww2bdq0yev9lpxvp9OpWbNmKSEhQV26dNHIkSN14sSJoMdfX1+vxx57TH369FGXLl2UnJysiRMn6uuvv/b6jCFDhjT6mYwfPz7o8Ust+31pr+dfUpP/F2w2m/74xz96+gTz/MN3IZnUW/tIu2DasWOHZs6cqY8++kiFhYU6f/68srOzdebMGa9+d9xxh8rLyz3bli1bghRxYzfeeKNXbIcOHfK89/zzz2vhwoVavHix9u7dK4fDodtuu001NTVBjPj/7N271yv2hps73HvvvZ4+7encnzlzRn379tXixYubfL8l53vOnDnauHGj1q1bp927d6u2tlZ33XWXXC5XUOM/e/as9u/fryeffFL79+/Xhg0b9Omnn2rkyJGN+k6bNs3rZ7Js2bKAxy5d+vxLl/59aa/nX5JX3OXl5VqxYoVsNlujp4AF6/zDD1r86Jd25KabbjLTp0/3arv++uvN448/HqSIWq6ystJIMjt27PC0TZo0ydx9993BC6oZCxYsMH379m3yPbfbbRwOh3nuuec8befOnTNxcXHm1VdfvUwRts7vfvc707NnT+N2u40x7fvcSzIbN270vG7J+f72229NZGSkWbdunafPV199ZTp06GDee++9yxa7MY3jb8r//M//GEnm+PHjnrZbb73V/O53vwtscC3QVPyX+n0JtfN/9913m2HDhnm1tZfzj7YJuUq9rq5ORUVFys7O9mrPzs7Wnj17ghRVy1VVVUmS4uPjvdq3b9+u7t27q3fv3po2bZoqKyuDEV6TiouLlZycrLS0NI0fP15ffPGFJKmkpEQVFRVePwu73a5bb721Xf4s6urq9MYbb+jBBx/0ekBCez73P9SS811UVKT6+nqvPsnJyUpPT2+XP5OqqirZbDZdccUVXu1r165VQkKCbrzxRs2bN6/djPxIzf++hNL5/+abb7R582ZNmTKl0Xvt+fyjeSH3lLa2PNKuvTDGKDc3Vz//+c+Vnp7uac/JydG9996r1NRUlZSU6Mknn9SwYcNUVFQU9Fs43nzzzVqzZo169+6tb775Rs8884wGDhyoI0eOeM53Uz+L48ePByPcZm3atEnffvutJk+e7Glrz+f+x1pyvisqKhQVFaUrr7yyUZ/29v/j3LlzevzxxzVhwgSvp4Tdf//9SktLk8Ph0OHDh5WXl6ePP/640X2xg+FSvy+hdP5Xr16tmJgYjR492qu9PZ9/XFrIJfUGrX2kXXvw8MMP6x//+Id2797t1T5u3DjPv9PT05WZmanU1FRt3ry50X+4yy0nJ8fz7z59+igrK0s9e/bU6tWrPQuEQuVnsXz5cuXk5Hg9v7k9n/uLacv5bm8/k/r6eo0fP15ut1tLlizxem/atGmef6enp6tXr17KzMzU/v371b9//8sdqpe2/r60t/MvSStWrND999/f6LGk7fn849JCbvi9LY+0aw9mzZqld955Rx9++KGuueaaZvsmJSUpNTVVxcXFlym6luvSpYv69Omj4uJizyr4UPhZHD9+XNu2bdPUqVOb7deez31LzrfD4VBdXZ1Onz590T7BVl9fr7Fjx6qkpESFhYWXfJZ3//79FRkZ2S5/Jj/+fQmF8y9Ju3bt0ieffHLJ/w9S+z7/aCzkknpbHmkXTMYYPfzww9qwYYM++OADpaWlXXKfU6dOqaysTElJSZchwtZxOp06duyYkpKSPEN0P/xZ1NXVaceOHe3uZ7Fy5Up1795dw4cPb7Zfez73LTnfGRkZioyM9OpTXl6uw4cPt4ufSUNCLy4u1rZt21r0nOgjR46ovr6+Xf5Mfvz70t7Pf4Ply5crIyNDffv2vWTf9nz+0YQgLtJrs3Xr1pnIyEizfPlyc/ToUTNnzhzTpUsX8+WXXwY7tEZ++9vfmri4OLN9+3ZTXl7u2c6ePWuMMaampsbMnTvX7Nmzx5SUlJgPP/zQZGVlmauvvtpUV1cHOXpj5s6da7Zv326++OIL89FHH5m77rrLxMTEeM71c889Z+Li4syGDRvMoUOHzH333WeSkpLaRewNXC6X6dGjh3nssce82tvjua+pqTEHDhwwBw4cMJLMwoULzYEDBzyrw1tyvqdPn26uueYas23bNrN//34zbNgw07dvX3P+/Pmgxl9fX29GjhxprrnmGnPw4EGv/w9Op9MYY8xnn31mnnrqKbN3715TUlJiNm/ebK6//nrTr1+/oMff0t+X9nr+G1RVVZnOnTubpUuXNto/2OcfvgvJpG6MMa+88opJTU01UVFRpn///l6XiLUnkprcVq5caYwx5uzZsyY7O9tcddVVJjIy0vTo0cNMmjTJlJaWBjfw740bN84kJSWZyMhIk5ycbEaPHm2OHDnied/tdpsFCxYYh8Nh7Ha7ueWWW8yhQ4eCGHFjW7duNZLMJ5984tXeHs/9hx9+2OTvy6RJk4wxLTvf3333nXn44YdNfHy86dSpk7nrrrsu23dqLv6SkpKL/n/48MMPjTHGlJaWmltuucXEx8ebqKgo07NnTzN79mxz6tSpoMff0t+X9nr+Gyxbtsx06tTJfPvtt432D/b5h+949CoAABYRcnPqAACgaSR1AAAsgqQOAIBFkNQBALAIkjoAABZBUgcAwCJI6gAAWARJHQAAiyCpAwBgESR1AAAsgqQOAIBF/H+0lim/LhRfzgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.randn(3, 768, 256)\n",
    "x[0, 600:, :]=0\n",
    "res = encoder(x)\n",
    "print(res[0])\n",
    "\n",
    "loss, hat, binary = model(x, masking_ratio=0.75, return_preds=True)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mask = create_attention_mask_from_padding(x)[0]\n",
    "# new_mask = new_mask.unsqueeze(1)\n",
    "\n",
    "p2d = (10,0, 0, 10)\n",
    "\n",
    "res = F.pad((new_mask[None, :]).to(torch.int), p2d, \"replicate\", 0).to(torch.bool)\n",
    "\n",
    "plt.imshow(res[0])\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5612,  0.0926,  1.8405,  ...,  0.3960,  0.0213, -0.7288],\n",
      "        [-0.5617,  0.0836,  1.8530,  ...,  0.3962,  0.0192, -0.7206],\n",
      "        [-0.5660,  0.0904,  1.8564,  ...,  0.3985,  0.0316, -0.7168],\n",
      "        ...,\n",
      "        [-0.5548, -0.0663,  1.9987,  ...,  0.5928,  0.2119, -0.7140],\n",
      "        [-0.5422, -0.0745,  1.9871,  ...,  0.6059,  0.2068, -0.7229],\n",
      "        [-0.5333, -0.0877,  1.9877,  ...,  0.6092,  0.2053, -0.7278]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "attn_mask_unmasked torch.Size([3, 1, 192, 192]) tensor([[[ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         ...,\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False]]])\n",
      "TOKENS AFTER ENCODER tensor([[-0.4848,  0.4451,  1.6810,  ...,  0.1693, -0.0746, -0.0506],\n",
      "        [-0.3261,  0.3938,  1.6440,  ...,  0.1168, -0.0298, -0.1103],\n",
      "        [-0.2725,  0.3834,  1.6295,  ...,  0.1474, -0.0101, -0.1203],\n",
      "        ...,\n",
      "        [-0.4569,  0.0470, -0.2386,  ..., -0.6901,  0.9369,  0.0377],\n",
      "        [-0.4680, -0.0582, -0.3537,  ..., -0.6273,  0.9243,  0.1170],\n",
      "        [-0.5141, -0.2585, -0.2594,  ..., -0.5268,  0.9402,  0.0230]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [3, 196, 256] cannot be broadcast to indexing result of shape [3, 192, 256]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[200], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m res \u001b[38;5;241m=\u001b[39m encoder(x)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(res[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m loss, hat, binary \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasking_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_preds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[195], line 377\u001b[0m, in \u001b[0;36mSimpleMAE.forward\u001b[0;34m(self, x, targets, date_info, masking_ratio, return_preds)\u001b[0m\n\u001b[1;32m    374\u001b[0m unmasked_decoder_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39memb(tokens)\n\u001b[1;32m    376\u001b[0m decoder_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(b, t, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 377\u001b[0m \u001b[43mdecoder_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munmasked_indices\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m unmasked_decoder_tokens\n\u001b[1;32m    378\u001b[0m decoder_tokens[batch_range, masked_indices] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_token\n\u001b[1;32m    380\u001b[0m decoder_pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_pos_emb(torch\u001b[38;5;241m.\u001b[39mcat([unmasked_indices, masked_indices], \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [3, 196, 256] cannot be broadcast to indexing result of shape [3, 192, 256]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.randn(3, 768, 256)\n",
    "x[0, 600:, :]=0\n",
    "res = encoder(x)\n",
    "print(res[0])\n",
    "\n",
    "loss, hat, binary = model(x, masking_ratio=0.75, return_preds=True)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m      2\u001b[0m x[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43maa\u001b[49m\n\u001b[1;32m      4\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m500\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m      5\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aa' is not defined"
     ]
    }
   ],
   "source": [
    "x = torch.ones(16, 500, 256)\n",
    "x[0, -100:] = 0\n",
    "aa\n",
    "attn_mask = torch.ones(500, 500).to(torch.bool)\n",
    "attn_mask = attn_mask.expand(16, -1, -1)\n",
    "\n",
    "attn_mask = create_attention_mask_from_padding(x)\n",
    "\n",
    "plt.imshow(attn_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 500, 500])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# batch_size, seq_length, feature_dim = 16, 500, 256\n",
    "# x = torch.ones(batch_size, seq_length, feature_dim)\n",
    "# x[0, -100:] = 0  # Pad the last 100 tokens of the first sequence\n",
    "\n",
    "# # Get the updated attention mask\n",
    "# attn_mask = update_attention_mask(x)\n",
    "\n",
    "# print(attn_mask.shape)  # Output the shape of the updated mask to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(16, 500, 256)\n",
    "x[0, -100:] = 0  # Pad the last 100 tokens of the first sequence\n",
    "\n",
    "is_padded = (x == 0).all(dim=2)\n",
    "\n",
    "batch_range = torch.arange(16, device=x.device)[:, None]\n",
    "num_masked = 100\n",
    "rand_indices = torch.rand(16, 500, device = x.device).argsort(dim=-1) # get idxs of random values.\n",
    "masked_indices, unmasked_indices = rand_indices[:, :num_masked], rand_indices[:, num_masked:]\n",
    "\n",
    "mask_valid = ~is_padded[batch_range, masked_indices]\n",
    "valid_masked_indices = masked_indices[mask_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pred_masked = x[batch_range, masked_indices]\n",
    "tokens_real_masked = x[batch_range, masked_indices]\n",
    "\n",
    "# #let's calculate loss on masked and not padded tokens.\n",
    "mask_valid = ~is_padded[batch_range, masked_indices]\n",
    "loss_tensor = F.mse_loss(tokens_pred_masked, tokens_real_masked, reduction='none')\n",
    "loss_real_values = loss_tensor[mask_valid.nonzero(as_tuple=True)]\n",
    "recon_loss = torch.mean(loss_real_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
